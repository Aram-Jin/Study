{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                            premise  \\\n",
      "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
      "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
      "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
      "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
      "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
      "\n",
      "                                hypothesis          label  \n",
      "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
      "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
      "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
      "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
      "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24998 entries, 0 to 24997\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       24998 non-null  int64 \n",
      " 1   premise     24998 non-null  object\n",
      " 2   hypothesis  24998 non-null  object\n",
      " 3   label       24998 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 781.3+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666 entries, 0 to 1665\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1666 non-null   int64 \n",
      " 1   premise     1666 non-null   object\n",
      " 2   hypothesis  1666 non-null   object\n",
      " 3   label       1666 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 52.2+ KB\n",
      "None\n",
      "Train Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Test Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Train Label: \n",
      "entailment       8561\n",
      "contradiction    8489\n",
      "neutral          7948\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Label: \n",
      "answer    1666\n",
      "Name: label, dtype: int64\n",
      "Train Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Test Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Premise Length:  90\n",
      "Min Premise Length:  19\n",
      "Mean Premise Lenght:  45.406552524201935 \n",
      "\n",
      "Max Hypothesis Length:  103\n",
      "Min Hypothesis Length:  5\n",
      "Mean Hypothesis Lenght:  24.924433954716378\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bitcamp\\AppData\\Local\\Temp/ipykernel_13504/1563639221.py:63: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "C:\\Users\\bitcamp\\AppData\\Local\\Temp/ipykernel_13504/1563639221.py:64: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
      "C:\\Users\\bitcamp\\AppData\\Local\\Temp/ipykernel_13504/1563639221.py:67: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "C:\\Users\\bitcamp\\AppData\\Local\\Temp/ipykernel_13504/1563639221.py:68: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tensor([    0, 30941,  2116, 11753,  5875, 12989,  2079, 14548,  2179,  3756,\n",
      "         6941,  1510,  2103,  2291,  6911,  2522,  1432,  2348,  2284,  2052,\n",
      "         7245,  3803,     2, 12989,  2145,  1510,  2103,  2291,  2073,  4273,\n",
      "         2470,  1536,  2052,  1415,  2062,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[CLS] 이정재가 삼성그룹 부회장 이재용의 전처인 대상그룹 임세령 상무와 열애중이라고 합니다 [SEP] 이재용과 임세령은 결혼한 적이 없다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "19998\n",
      "{'input_ids': tensor([    0, 22748,  2255, 19418, 12830,  2256,  2079, 16878,  6385,  7873,\n",
      "         2170,  3708,  4713,  2999,  7285,  7389,  2371,  2062,     2, 22748,\n",
      "         2255, 19418, 12830,  2256,  2259, 17291,  2318,  7873,  2371,  2062,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'label': tensor(0)}\n",
      "[CLS] 안드레스 이니에스타의 갑작스런 스페인 귀국에 일본 축구팬들이 당황했다 [SEP] 안드레스 이니에스타는 갑작스럽게 귀국했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH =  \"D:/_data/dacon/open/\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(PATH, 'train_data.csv'), encoding='utf-8')\n",
    "test = pd.read_csv(os.path.join(PATH, 'test_data.csv'), encoding='utf-8')\n",
    "\n",
    "print(train.head())\n",
    "print(train.info(), end='\\n\\n')\n",
    "print(test.info())\n",
    "print('Train Columns: ', train.columns)\n",
    "print('Test Columns: ', test.columns)\n",
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')\n",
    "print('Test Label: ', test['label'].value_counts(), sep='\\n')\n",
    "print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n",
    "print('Test Null: ', test.isnull().sum(), sep='\\n')\n",
    "\n",
    "feature = train['label']\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Label Count', fontsize=20)\n",
    "\n",
    "temp = feature.value_counts()\n",
    "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
    "plt.text(-0.05, temp.values[0]+20, s=temp.values[0])\n",
    "plt.text(0.95, temp.values[1]+20, s=temp.values[1])\n",
    "plt.text(1.95, temp.values[2]+20, s=temp.values[2])\n",
    "\n",
    "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "plt.show() # 그래프 나타내기\n",
    "\n",
    "max_len = np.max(train['premise'].str.len())\n",
    "min_len = np.min(train['premise'].str.len())\n",
    "mean_len = np.mean(train['premise'].str.len())\n",
    "\n",
    "print('Max Premise Length: ', max_len)\n",
    "print('Min Premise Length: ', min_len)\n",
    "print('Mean Premise Lenght: ', mean_len, '\\n')\n",
    "\n",
    "max_len = np.max(train['hypothesis'].str.len())\n",
    "min_len = np.min(train['hypothesis'].str.len())\n",
    "mean_len = np.mean(train['hypothesis'].str.len())\n",
    "\n",
    "print('Max Hypothesis Length: ', max_len)\n",
    "print('Min Hypothesis Length: ', min_len)\n",
    "print('Mean Hypothesis Lenght: ', mean_len)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Premise Length', fontsize=20)\n",
    "\n",
    "plt.hist(train['premise'].str.len(), alpha=0.5, color='orange')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)\n",
    "\n",
    "train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "\n",
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))\n",
    "\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)\n",
    "\n",
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(19997))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validation을 위한 metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=14,\n",
    "    per_device_train_batch_size=6,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19998\n",
      "  Num Epochs = 14\n",
      "  Instantaneous batch size per device = 6\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46662\n",
      "  1%|          | 500/46662 [02:16<3:22:07,  3.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1449, 'learning_rate': 4.946423213749947e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  1%|          | 500/46662 [03:05<3:22:07,  3.81it/s]Saving model checkpoint to ./result\\checkpoint-500\n",
      "Configuration saved in ./result\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1187105178833008, 'eval_accuracy': 0.3396, 'eval_runtime': 48.8519, 'eval_samples_per_second': 102.35, 'eval_steps_per_second': 12.794, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-19500] due to args.save_total_limit\n",
      "  2%|▏         | 1000/46662 [05:47<3:26:21,  3.69it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.127, 'learning_rate': 4.892846427499893e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  2%|▏         | 1000/46662 [06:45<3:26:21,  3.69it/s]Saving model checkpoint to ./result\\checkpoint-1000\n",
      "Configuration saved in ./result\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1318696737289429, 'eval_accuracy': 0.318, 'eval_runtime': 57.9597, 'eval_samples_per_second': 86.267, 'eval_steps_per_second': 10.783, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-26500] due to args.save_total_limit\n",
      "  3%|▎         | 1500/46662 [09:22<3:21:50,  3.73it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1217, 'learning_rate': 4.8392696412498395e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  3%|▎         | 1500/46662 [10:12<3:21:50,  3.73it/s]Saving model checkpoint to ./result\\checkpoint-1500\n",
      "Configuration saved in ./result\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1042780876159668, 'eval_accuracy': 0.318, 'eval_runtime': 49.8669, 'eval_samples_per_second': 100.267, 'eval_steps_per_second': 12.533, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-27000] due to args.save_total_limit\n",
      "  4%|▍         | 2000/46662 [12:49<3:20:01,  3.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1196, 'learning_rate': 4.785692854999786e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  4%|▍         | 2000/46662 [13:39<3:20:01,  3.72it/s]Saving model checkpoint to ./result\\checkpoint-2000\n",
      "Configuration saved in ./result\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1183518171310425, 'eval_accuracy': 0.3424, 'eval_runtime': 49.9799, 'eval_samples_per_second': 100.04, 'eval_steps_per_second': 12.505, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-27500] due to args.save_total_limit\n",
      "  5%|▌         | 2500/46662 [16:15<3:18:09,  3.71it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1173, 'learning_rate': 4.732116068749732e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  5%|▌         | 2500/46662 [17:05<3:18:09,  3.71it/s]Saving model checkpoint to ./result\\checkpoint-2500\n",
      "Configuration saved in ./result\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.102891445159912, 'eval_accuracy': 0.3424, 'eval_runtime': 50.0356, 'eval_samples_per_second': 99.929, 'eval_steps_per_second': 12.491, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-28000] due to args.save_total_limit\n",
      "  6%|▋         | 3000/46662 [19:38<3:15:13,  3.73it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1137, 'learning_rate': 4.678539282499679e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  6%|▋         | 3000/46662 [20:28<3:15:13,  3.73it/s]Saving model checkpoint to ./result\\checkpoint-3000\n",
      "Configuration saved in ./result\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.104387879371643, 'eval_accuracy': 0.3424, 'eval_runtime': 50.0549, 'eval_samples_per_second': 99.89, 'eval_steps_per_second': 12.486, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-500] due to args.save_total_limit\n",
      "  8%|▊         | 3500/46662 [23:03<3:13:52,  3.71it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1161, 'learning_rate': 4.6249624962496254e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  8%|▊         | 3500/46662 [23:53<3:13:52,  3.71it/s]Saving model checkpoint to ./result\\checkpoint-3500\n",
      "Configuration saved in ./result\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1013422012329102, 'eval_accuracy': 0.3396, 'eval_runtime': 50.1223, 'eval_samples_per_second': 99.756, 'eval_steps_per_second': 12.469, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-1000] due to args.save_total_limit\n",
      "  9%|▊         | 4000/46662 [26:29<3:11:56,  3.70it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1154, 'learning_rate': 4.5713857099995714e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  9%|▊         | 4000/46662 [27:19<3:11:56,  3.70it/s]Saving model checkpoint to ./result\\checkpoint-4000\n",
      "Configuration saved in ./result\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1090044975280762, 'eval_accuracy': 0.3396, 'eval_runtime': 50.1462, 'eval_samples_per_second': 99.708, 'eval_steps_per_second': 12.464, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-1500] due to args.save_total_limit\n",
      " 10%|▉         | 4500/46662 [29:49<3:08:49,  3.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1134, 'learning_rate': 4.517808923749518e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 10%|▉         | 4500/46662 [30:42<3:08:49,  3.72it/s]Saving model checkpoint to ./result\\checkpoint-4500\n",
      "Configuration saved in ./result\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1002243757247925, 'eval_accuracy': 0.3424, 'eval_runtime': 52.8094, 'eval_samples_per_second': 94.68, 'eval_steps_per_second': 11.835, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-4500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-2000] due to args.save_total_limit\n",
      " 11%|█         | 5000/46662 [33:21<3:04:15,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1114, 'learning_rate': 4.464232137499465e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 11%|█         | 5000/46662 [34:10<3:04:15,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-5000\n",
      "Configuration saved in ./result\\checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1123249530792236, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5105, 'eval_samples_per_second': 100.989, 'eval_steps_per_second': 12.624, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-5000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-2500] due to args.save_total_limit\n",
      " 12%|█▏        | 5500/46662 [36:46<3:02:03,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1061, 'learning_rate': 4.410655351249411e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|█▏        | 5500/46662 [37:36<3:02:03,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-5500\n",
      "Configuration saved in ./result\\checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1098495721817017, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5252, 'eval_samples_per_second': 100.959, 'eval_steps_per_second': 12.62, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-5500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-3000] due to args.save_total_limit\n",
      " 13%|█▎        | 6000/46662 [40:17<3:00:44,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1133, 'learning_rate': 4.3570785649993573e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 13%|█▎        | 6000/46662 [41:07<3:00:44,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-6000\n",
      "Configuration saved in ./result\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1095460653305054, 'eval_accuracy': 0.318, 'eval_runtime': 49.8433, 'eval_samples_per_second': 100.314, 'eval_steps_per_second': 12.539, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-6000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-3500] due to args.save_total_limit\n",
      " 14%|█▍        | 6500/46662 [43:42<2:58:33,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1117, 'learning_rate': 4.303501778749304e-05, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 14%|█▍        | 6500/46662 [44:32<2:58:33,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-6500\n",
      "Configuration saved in ./result\\checkpoint-6500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0993022918701172, 'eval_accuracy': 0.3424, 'eval_runtime': 49.8195, 'eval_samples_per_second': 100.362, 'eval_steps_per_second': 12.545, 'epoch': 1.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-6500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-4000] due to args.save_total_limit\n",
      " 15%|█▌        | 7000/46662 [47:06<2:55:33,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1061, 'learning_rate': 4.24992499249925e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 15%|█▌        | 7000/46662 [47:56<2:55:33,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-7000\n",
      "Configuration saved in ./result\\checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.100595235824585, 'eval_accuracy': 0.3424, 'eval_runtime': 49.7952, 'eval_samples_per_second': 100.411, 'eval_steps_per_second': 12.551, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-7000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-4500] due to args.save_total_limit\n",
      " 16%|█▌        | 7500/46662 [50:25<2:53:04,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1047, 'learning_rate': 4.1963482062491966e-05, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 16%|█▌        | 7500/46662 [51:15<2:53:04,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-7500\n",
      "Configuration saved in ./result\\checkpoint-7500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.103683590888977, 'eval_accuracy': 0.318, 'eval_runtime': 49.6523, 'eval_samples_per_second': 100.7, 'eval_steps_per_second': 12.588, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-7500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-5000] due to args.save_total_limit\n",
      " 17%|█▋        | 8000/46662 [53:49<2:50:47,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1058, 'learning_rate': 4.142771419999143e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 17%|█▋        | 8000/46662 [54:38<2:50:47,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-8000\n",
      "Configuration saved in ./result\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1001402139663696, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5702, 'eval_samples_per_second': 100.867, 'eval_steps_per_second': 12.608, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-8000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-5500] due to args.save_total_limit\n",
      " 18%|█▊        | 8500/46662 [57:15<2:47:29,  3.80it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1068, 'learning_rate': 4.089194633749089e-05, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 18%|█▊        | 8500/46662 [58:05<2:47:29,  3.80it/s]Saving model checkpoint to ./result\\checkpoint-8500\n",
      "Configuration saved in ./result\\checkpoint-8500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1081258058547974, 'eval_accuracy': 0.318, 'eval_runtime': 49.5772, 'eval_samples_per_second': 100.853, 'eval_steps_per_second': 12.607, 'epoch': 2.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-8500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-6000] due to args.save_total_limit\n",
      " 19%|█▉        | 9000/46662 [1:00:42<2:46:33,  3.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1091, 'learning_rate': 4.035617847499036e-05, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 19%|█▉        | 9000/46662 [1:01:32<2:46:33,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-9000\n",
      "Configuration saved in ./result\\checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098116159439087, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5897, 'eval_samples_per_second': 100.827, 'eval_steps_per_second': 12.603, 'epoch': 2.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-9000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-6500] due to args.save_total_limit\n",
      " 20%|██        | 9500/46662 [1:04:08<2:45:16,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1069, 'learning_rate': 3.9820410612489825e-05, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 20%|██        | 9500/46662 [1:04:57<2:45:16,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-9500\n",
      "Configuration saved in ./result\\checkpoint-9500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0984965562820435, 'eval_accuracy': 0.3396, 'eval_runtime': 49.324, 'eval_samples_per_second': 101.37, 'eval_steps_per_second': 12.671, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-9500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-9500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-9500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-7000] due to args.save_total_limit\n",
      " 21%|██▏       | 10000/46662 [1:07:33<2:40:34,  3.81it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1048, 'learning_rate': 3.9284642749989285e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 21%|██▏       | 10000/46662 [1:08:23<2:40:34,  3.81it/s]Saving model checkpoint to ./result\\checkpoint-10000\n",
      "Configuration saved in ./result\\checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.101319670677185, 'eval_accuracy': 0.3424, 'eval_runtime': 49.3968, 'eval_samples_per_second': 101.221, 'eval_steps_per_second': 12.653, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-10000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-7500] due to args.save_total_limit\n",
      " 23%|██▎       | 10500/46662 [1:10:56<2:39:41,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.107, 'learning_rate': 3.874887488748875e-05, 'epoch': 3.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 23%|██▎       | 10500/46662 [1:11:45<2:39:41,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-10500\n",
      "Configuration saved in ./result\\checkpoint-10500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.109466791152954, 'eval_accuracy': 0.318, 'eval_runtime': 49.4652, 'eval_samples_per_second': 101.081, 'eval_steps_per_second': 12.635, 'epoch': 3.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-10500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-10500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-10500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-8000] due to args.save_total_limit\n",
      " 24%|██▎       | 11000/46662 [1:14:20<2:37:12,  3.78it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1051, 'learning_rate': 3.821310702498822e-05, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 24%|██▎       | 11000/46662 [1:15:09<2:37:12,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-11000\n",
      "Configuration saved in ./result\\checkpoint-11000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0999585390090942, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4272, 'eval_samples_per_second': 101.159, 'eval_steps_per_second': 12.645, 'epoch': 3.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-11000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-8500] due to args.save_total_limit\n",
      " 25%|██▍       | 11500/46662 [1:17:43<2:35:23,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1074, 'learning_rate': 3.767733916248768e-05, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 25%|██▍       | 11500/46662 [1:18:33<2:35:23,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-11500\n",
      "Configuration saved in ./result\\checkpoint-11500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1026501655578613, 'eval_accuracy': 0.318, 'eval_runtime': 49.4501, 'eval_samples_per_second': 101.112, 'eval_steps_per_second': 12.639, 'epoch': 3.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-11500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-11500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-11500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-9500] due to args.save_total_limit\n",
      " 26%|██▌       | 12000/46662 [1:21:10<2:33:33,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1091, 'learning_rate': 3.7141571299987145e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 26%|██▌       | 12000/46662 [1:21:59<2:33:33,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-12000\n",
      "Configuration saved in ./result\\checkpoint-12000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0989038944244385, 'eval_accuracy': 0.3396, 'eval_runtime': 49.436, 'eval_samples_per_second': 101.141, 'eval_steps_per_second': 12.643, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-12000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-10000] due to args.save_total_limit\n",
      " 27%|██▋       | 12500/46662 [1:24:36<2:31:02,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1014, 'learning_rate': 3.660580343748661e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 27%|██▋       | 12500/46662 [1:25:26<2:31:02,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-12500\n",
      "Configuration saved in ./result\\checkpoint-12500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1108087301254272, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4492, 'eval_samples_per_second': 101.114, 'eval_steps_per_second': 12.639, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-12500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-12500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-12500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-10500] due to args.save_total_limit\n",
      " 28%|██▊       | 13000/46662 [1:28:01<2:28:21,  3.78it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1039, 'learning_rate': 3.607003557498607e-05, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 28%|██▊       | 13000/46662 [1:28:51<2:28:21,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-13000\n",
      "Configuration saved in ./result\\checkpoint-13000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1006516218185425, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4949, 'eval_samples_per_second': 101.021, 'eval_steps_per_second': 12.628, 'epoch': 3.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-13000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-11000] due to args.save_total_limit\n",
      " 29%|██▉       | 13500/46662 [1:31:28<2:31:24,  3.65it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1034, 'learning_rate': 3.553426771248554e-05, 'epoch': 4.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 29%|██▉       | 13500/46662 [1:32:18<2:31:24,  3.65it/s]Saving model checkpoint to ./result\\checkpoint-13500\n",
      "Configuration saved in ./result\\checkpoint-13500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1197620630264282, 'eval_accuracy': 0.3396, 'eval_runtime': 49.9584, 'eval_samples_per_second': 100.083, 'eval_steps_per_second': 12.51, 'epoch': 4.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-13500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-13500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-13500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-11500] due to args.save_total_limit\n",
      " 30%|███       | 14000/46662 [1:35:35<2:26:23,  3.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.106, 'learning_rate': 3.4998499849985e-05, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 30%|███       | 14000/46662 [1:36:26<2:26:23,  3.72it/s]Saving model checkpoint to ./result\\checkpoint-14000\n",
      "Configuration saved in ./result\\checkpoint-14000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1031407117843628, 'eval_accuracy': 0.3424, 'eval_runtime': 50.6223, 'eval_samples_per_second': 98.771, 'eval_steps_per_second': 12.346, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-14000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-12000] due to args.save_total_limit\n",
      " 31%|███       | 14500/46662 [1:39:03<2:31:24,  3.54it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1066, 'learning_rate': 3.4462731987484464e-05, 'epoch': 4.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 31%|███       | 14500/46662 [1:39:56<2:31:24,  3.54it/s]Saving model checkpoint to ./result\\checkpoint-14500\n",
      "Configuration saved in ./result\\checkpoint-14500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1004033088684082, 'eval_accuracy': 0.3396, 'eval_runtime': 53.2833, 'eval_samples_per_second': 93.838, 'eval_steps_per_second': 11.73, 'epoch': 4.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-14500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-14500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-14500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-12500] due to args.save_total_limit\n",
      " 32%|███▏      | 15000/46662 [1:42:34<2:20:46,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1051, 'learning_rate': 3.392696412498393e-05, 'epoch': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 32%|███▏      | 15000/46662 [1:43:23<2:20:46,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-15000\n",
      "Configuration saved in ./result\\checkpoint-15000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1054798364639282, 'eval_accuracy': 0.3424, 'eval_runtime': 49.6431, 'eval_samples_per_second': 100.719, 'eval_steps_per_second': 12.59, 'epoch': 4.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-15000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-13000] due to args.save_total_limit\n",
      " 33%|███▎      | 15500/46662 [1:45:58<2:17:48,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1054, 'learning_rate': 3.339119626248339e-05, 'epoch': 4.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 33%|███▎      | 15500/46662 [1:46:48<2:17:48,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-15500\n",
      "Configuration saved in ./result\\checkpoint-15500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1094074249267578, 'eval_accuracy': 0.3396, 'eval_runtime': 49.6542, 'eval_samples_per_second': 100.696, 'eval_steps_per_second': 12.587, 'epoch': 4.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-15500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-15500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-15500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-13500] due to args.save_total_limit\n",
      " 34%|███▍      | 16000/46662 [1:49:23<2:15:57,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1045, 'learning_rate': 3.2855428399982856e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 34%|███▍      | 16000/46662 [1:50:13<2:15:57,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-16000\n",
      "Configuration saved in ./result\\checkpoint-16000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0985450744628906, 'eval_accuracy': 0.3424, 'eval_runtime': 49.6949, 'eval_samples_per_second': 100.614, 'eval_steps_per_second': 12.577, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-16000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-14000] due to args.save_total_limit\n",
      " 35%|███▌      | 16500/46662 [1:52:46<2:13:42,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.107, 'learning_rate': 3.231966053748232e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 35%|███▌      | 16500/46662 [1:53:36<2:13:42,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-16500\n",
      "Configuration saved in ./result\\checkpoint-16500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0986857414245605, 'eval_accuracy': 0.3396, 'eval_runtime': 49.7075, 'eval_samples_per_second': 100.588, 'eval_steps_per_second': 12.574, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-16500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-16500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-16500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-14500] due to args.save_total_limit\n",
      " 36%|███▋      | 17000/46662 [1:56:09<2:10:54,  3.78it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1056, 'learning_rate': 3.178389267498178e-05, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 36%|███▋      | 17000/46662 [1:56:58<2:10:54,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-17000\n",
      "Configuration saved in ./result\\checkpoint-17000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0998154878616333, 'eval_accuracy': 0.3396, 'eval_runtime': 49.6003, 'eval_samples_per_second': 100.806, 'eval_steps_per_second': 12.601, 'epoch': 5.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-17000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-15000] due to args.save_total_limit\n",
      " 38%|███▊      | 17500/46662 [1:59:34<2:09:29,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1039, 'learning_rate': 3.124812481248125e-05, 'epoch': 5.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 38%|███▊      | 17500/46662 [2:00:23<2:09:29,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-17500\n",
      "Configuration saved in ./result\\checkpoint-17500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0996297597885132, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5838, 'eval_samples_per_second': 100.839, 'eval_steps_per_second': 12.605, 'epoch': 5.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-17500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-17500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-17500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-15500] due to args.save_total_limit\n",
      " 39%|███▊      | 18000/46662 [2:02:58<2:06:32,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1023, 'learning_rate': 3.0712356949980716e-05, 'epoch': 5.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 39%|███▊      | 18000/46662 [2:03:47<2:06:32,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-18000\n",
      "Configuration saved in ./result\\checkpoint-18000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1010690927505493, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5487, 'eval_samples_per_second': 100.911, 'eval_steps_per_second': 12.614, 'epoch': 5.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-18000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-18000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-18000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-16000] due to args.save_total_limit\n",
      " 40%|███▉      | 18500/46662 [2:06:21<2:04:28,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1051, 'learning_rate': 3.017658908748018e-05, 'epoch': 5.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 40%|███▉      | 18500/46662 [2:07:10<2:04:28,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-18500\n",
      "Configuration saved in ./result\\checkpoint-18500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.100846529006958, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5525, 'eval_samples_per_second': 100.903, 'eval_steps_per_second': 12.613, 'epoch': 5.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-18500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-18500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-18500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-16500] due to args.save_total_limit\n",
      " 41%|████      | 19000/46662 [2:09:44<2:02:26,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1035, 'learning_rate': 2.9640821224979642e-05, 'epoch': 5.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 41%|████      | 19000/46662 [2:10:33<2:02:26,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-19000\n",
      "Configuration saved in ./result\\checkpoint-19000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0990328788757324, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5433, 'eval_samples_per_second': 100.922, 'eval_steps_per_second': 12.615, 'epoch': 5.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-19000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-19000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-19000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-17000] due to args.save_total_limit\n",
      " 42%|████▏     | 19500/46662 [2:13:07<2:00:17,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1022, 'learning_rate': 2.9105053362479105e-05, 'epoch': 5.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 42%|████▏     | 19500/46662 [2:13:57<2:00:17,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-19500\n",
      "Configuration saved in ./result\\checkpoint-19500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1115565299987793, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5827, 'eval_samples_per_second': 100.842, 'eval_steps_per_second': 12.605, 'epoch': 5.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-19500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-19500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-19500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-17500] due to args.save_total_limit\n",
      " 43%|████▎     | 20000/46662 [2:16:31<1:58:16,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1039, 'learning_rate': 2.856928549997857e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 43%|████▎     | 20000/46662 [2:17:20<1:58:16,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-20000\n",
      "Configuration saved in ./result\\checkpoint-20000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0984798669815063, 'eval_accuracy': 0.3424, 'eval_runtime': 49.501, 'eval_samples_per_second': 101.008, 'eval_steps_per_second': 12.626, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-20000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-18000] due to args.save_total_limit\n",
      " 44%|████▍     | 20500/46662 [2:19:55<1:55:33,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1033, 'learning_rate': 2.8033517637478035e-05, 'epoch': 6.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 44%|████▍     | 20500/46662 [2:20:45<1:55:33,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-20500\n",
      "Configuration saved in ./result\\checkpoint-20500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0988599061965942, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5841, 'eval_samples_per_second': 100.839, 'eval_steps_per_second': 12.605, 'epoch': 6.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-20500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-20500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-20500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-18500] due to args.save_total_limit\n",
      " 45%|████▌     | 21000/46662 [2:23:18<1:51:53,  3.82it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1015, 'learning_rate': 2.7497749774977498e-05, 'epoch': 6.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 45%|████▌     | 21000/46662 [2:24:08<1:51:53,  3.82it/s]Saving model checkpoint to ./result\\checkpoint-21000\n",
      "Configuration saved in ./result\\checkpoint-21000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983021259307861, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5983, 'eval_samples_per_second': 100.81, 'eval_steps_per_second': 12.601, 'epoch': 6.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-21000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-21000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-21000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-19000] due to args.save_total_limit\n",
      " 46%|████▌     | 21500/46662 [2:26:42<1:51:13,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1018, 'learning_rate': 2.6961981912476964e-05, 'epoch': 6.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 46%|████▌     | 21500/46662 [2:27:31<1:51:13,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-21500\n",
      "Configuration saved in ./result\\checkpoint-21500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1003570556640625, 'eval_accuracy': 0.3424, 'eval_runtime': 49.6281, 'eval_samples_per_second': 100.749, 'eval_steps_per_second': 12.594, 'epoch': 6.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-21500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-21500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-21500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-19500] due to args.save_total_limit\n",
      " 47%|████▋     | 22000/46662 [2:30:06<1:47:40,  3.82it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1025, 'learning_rate': 2.6426214049976427e-05, 'epoch': 6.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 47%|████▋     | 22000/46662 [2:30:56<1:47:40,  3.82it/s]Saving model checkpoint to ./result\\checkpoint-22000\n",
      "Configuration saved in ./result\\checkpoint-22000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098313331604004, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5373, 'eval_samples_per_second': 100.934, 'eval_steps_per_second': 12.617, 'epoch': 6.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-22000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-22000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-22000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-20000] due to args.save_total_limit\n",
      " 48%|████▊     | 22500/46662 [2:33:31<1:46:32,  3.78it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'learning_rate': 2.589044618747589e-05, 'epoch': 6.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 48%|████▊     | 22500/46662 [2:34:21<1:46:32,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-22500\n",
      "Configuration saved in ./result\\checkpoint-22500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983461141586304, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5771, 'eval_samples_per_second': 100.853, 'eval_steps_per_second': 12.607, 'epoch': 6.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-22500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-22500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-22500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-20500] due to args.save_total_limit\n",
      " 49%|████▉     | 23000/46662 [2:36:55<1:45:01,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1001, 'learning_rate': 2.5354678324975357e-05, 'epoch': 6.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 49%|████▉     | 23000/46662 [2:37:45<1:45:01,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-23000\n",
      "Configuration saved in ./result\\checkpoint-23000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1007862091064453, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5334, 'eval_samples_per_second': 100.942, 'eval_steps_per_second': 12.618, 'epoch': 6.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-23000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-23000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-23000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-21000] due to args.save_total_limit\n",
      " 50%|█████     | 23500/46662 [2:40:20<1:42:32,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'learning_rate': 2.481891046247482e-05, 'epoch': 7.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 50%|█████     | 23500/46662 [2:41:10<1:42:32,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-23500\n",
      "Configuration saved in ./result\\checkpoint-23500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1032313108444214, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5438, 'eval_samples_per_second': 100.921, 'eval_steps_per_second': 12.615, 'epoch': 7.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-23500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-23500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-23500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-21500] due to args.save_total_limit\n",
      " 51%|█████▏    | 24000/46662 [2:43:40<1:40:32,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1038, 'learning_rate': 2.4283142599974283e-05, 'epoch': 7.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 51%|█████▏    | 24000/46662 [2:44:30<1:40:32,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-24000\n",
      "Configuration saved in ./result\\checkpoint-24000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0980963706970215, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5289, 'eval_samples_per_second': 100.951, 'eval_steps_per_second': 12.619, 'epoch': 7.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-24000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-24000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-24000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-9000] due to args.save_total_limit\n",
      " 53%|█████▎    | 24500/46662 [2:47:04<1:38:26,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0999, 'learning_rate': 2.374737473747375e-05, 'epoch': 7.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 53%|█████▎    | 24500/46662 [2:47:53<1:38:26,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-24500\n",
      "Configuration saved in ./result\\checkpoint-24500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1139237880706787, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4871, 'eval_samples_per_second': 101.036, 'eval_steps_per_second': 12.63, 'epoch': 7.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-24500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-24500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-24500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-22000] due to args.save_total_limit\n",
      " 54%|█████▎    | 25000/46662 [2:50:28<1:36:09,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1026, 'learning_rate': 2.3211606874973213e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 54%|█████▎    | 25000/46662 [2:51:17<1:36:09,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-25000\n",
      "Configuration saved in ./result\\checkpoint-25000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098572015762329, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4725, 'eval_samples_per_second': 101.066, 'eval_steps_per_second': 12.633, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-25000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-25000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-25000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-22500] due to args.save_total_limit\n",
      " 55%|█████▍    | 25500/46662 [2:53:53<1:33:39,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1025, 'learning_rate': 2.2675839012472676e-05, 'epoch': 7.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 55%|█████▍    | 25500/46662 [2:54:43<1:33:39,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-25500\n",
      "Configuration saved in ./result\\checkpoint-25500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0985326766967773, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4737, 'eval_samples_per_second': 101.064, 'eval_steps_per_second': 12.633, 'epoch': 7.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-25500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-25500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-25500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-23000] due to args.save_total_limit\n",
      " 56%|█████▌    | 26000/46662 [2:57:18<1:31:19,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1017, 'learning_rate': 2.2140071149972143e-05, 'epoch': 7.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 56%|█████▌    | 26000/46662 [2:58:07<1:31:19,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-26000\n",
      "Configuration saved in ./result\\checkpoint-26000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0986448526382446, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5161, 'eval_samples_per_second': 100.977, 'eval_steps_per_second': 12.622, 'epoch': 7.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-26000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-26000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-26000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-23500] due to args.save_total_limit\n",
      " 57%|█████▋    | 26500/46662 [3:00:42<1:29:23,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1019, 'learning_rate': 2.1604303287471606e-05, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 57%|█████▋    | 26500/46662 [3:01:32<1:29:23,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-26500\n",
      "Configuration saved in ./result\\checkpoint-26500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0980784893035889, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5939, 'eval_samples_per_second': 100.819, 'eval_steps_per_second': 12.602, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-26500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-26500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-26500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-24000] due to args.save_total_limit\n",
      " 58%|█████▊    | 27000/46662 [3:04:07<1:27:02,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1011, 'learning_rate': 2.106853542497107e-05, 'epoch': 8.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 58%|█████▊    | 27000/46662 [3:04:57<1:27:02,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-27000\n",
      "Configuration saved in ./result\\checkpoint-27000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.102786660194397, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5586, 'eval_samples_per_second': 100.891, 'eval_steps_per_second': 12.611, 'epoch': 8.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-27000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-27000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-27000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-24500] due to args.save_total_limit\n",
      " 59%|█████▉    | 27500/46662 [3:07:33<1:24:10,  3.79it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1011, 'learning_rate': 2.0532767562470535e-05, 'epoch': 8.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 59%|█████▉    | 27500/46662 [3:08:23<1:24:10,  3.79it/s]Saving model checkpoint to ./result\\checkpoint-27500\n",
      "Configuration saved in ./result\\checkpoint-27500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1008203029632568, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5604, 'eval_samples_per_second': 100.887, 'eval_steps_per_second': 12.611, 'epoch': 8.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-27500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-27500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-27500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-25000] due to args.save_total_limit\n",
      " 60%|██████    | 28000/46662 [3:10:56<1:22:48,  3.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1, 'learning_rate': 1.999699969997e-05, 'epoch': 8.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 60%|██████    | 28000/46662 [3:11:46<1:22:48,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-28000\n",
      "Configuration saved in ./result\\checkpoint-28000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0984070301055908, 'eval_accuracy': 0.3424, 'eval_runtime': 49.6653, 'eval_samples_per_second': 100.674, 'eval_steps_per_second': 12.584, 'epoch': 8.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-28000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-28000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-28000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-25500] due to args.save_total_limit\n",
      " 61%|██████    | 28500/46662 [3:14:21<1:20:21,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.102, 'learning_rate': 1.946123183746946e-05, 'epoch': 8.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 61%|██████    | 28500/46662 [3:15:10<1:20:21,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-28500\n",
      "Configuration saved in ./result\\checkpoint-28500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0985349416732788, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5368, 'eval_samples_per_second': 100.935, 'eval_steps_per_second': 12.617, 'epoch': 8.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-28500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-28500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-28500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-26000] due to args.save_total_limit\n",
      " 62%|██████▏   | 29000/46662 [3:17:46<1:18:01,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1019, 'learning_rate': 1.8925463974968928e-05, 'epoch': 8.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 62%|██████▏   | 29000/46662 [3:18:36<1:18:01,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-29000\n",
      "Configuration saved in ./result\\checkpoint-29000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0988258123397827, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5136, 'eval_samples_per_second': 100.982, 'eval_steps_per_second': 12.623, 'epoch': 8.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-29000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-29000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-29000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-27000] due to args.save_total_limit\n",
      " 63%|██████▎   | 29500/46662 [3:21:10<1:16:17,  3.75it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1, 'learning_rate': 1.838969611246839e-05, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 63%|██████▎   | 29500/46662 [3:21:59<1:16:17,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-29500\n",
      "Configuration saved in ./result\\checkpoint-29500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1037935018539429, 'eval_accuracy': 0.318, 'eval_runtime': 49.4873, 'eval_samples_per_second': 101.036, 'eval_steps_per_second': 12.63, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-29500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-29500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-29500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-27500] due to args.save_total_limit\n",
      " 64%|██████▍   | 30000/46662 [3:24:35<1:13:38,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1002, 'learning_rate': 1.7853928249967854e-05, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 64%|██████▍   | 30000/46662 [3:25:24<1:13:38,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-30000\n",
      "Configuration saved in ./result\\checkpoint-30000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0995904207229614, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4501, 'eval_samples_per_second': 101.112, 'eval_steps_per_second': 12.639, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-30000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-30000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-30000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-28000] due to args.save_total_limit\n",
      " 65%|██████▌   | 30500/46662 [3:27:59<1:11:29,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1, 'learning_rate': 1.731816038746732e-05, 'epoch': 9.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 65%|██████▌   | 30500/46662 [3:28:49<1:11:29,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-30500\n",
      "Configuration saved in ./result\\checkpoint-30500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098681926727295, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4999, 'eval_samples_per_second': 101.01, 'eval_steps_per_second': 12.626, 'epoch': 9.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-30500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-30500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-30500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-28500] due to args.save_total_limit\n",
      " 66%|██████▋   | 31000/46662 [3:31:24<1:08:38,  3.80it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0999, 'learning_rate': 1.6782392524966784e-05, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 66%|██████▋   | 31000/46662 [3:32:14<1:08:38,  3.80it/s]Saving model checkpoint to ./result\\checkpoint-31000\n",
      "Configuration saved in ./result\\checkpoint-31000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098838448524475, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4683, 'eval_samples_per_second': 101.075, 'eval_steps_per_second': 12.634, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-31000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-31000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-31000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-29000] due to args.save_total_limit\n",
      " 68%|██████▊   | 31500/46662 [3:34:46<1:07:04,  3.77it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1007, 'learning_rate': 1.6246624662466247e-05, 'epoch': 9.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 68%|██████▊   | 31500/46662 [3:35:35<1:07:04,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-31500\n",
      "Configuration saved in ./result\\checkpoint-31500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1014741659164429, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5768, 'eval_samples_per_second': 100.854, 'eval_steps_per_second': 12.607, 'epoch': 9.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-31500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-31500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-31500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-29500] due to args.save_total_limit\n",
      " 69%|██████▊   | 32000/46662 [3:38:12<1:04:52,  3.77it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.099, 'learning_rate': 1.5710856799965714e-05, 'epoch': 9.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 69%|██████▊   | 32000/46662 [3:39:02<1:04:52,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-32000\n",
      "Configuration saved in ./result\\checkpoint-32000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983402729034424, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4762, 'eval_samples_per_second': 101.059, 'eval_steps_per_second': 12.632, 'epoch': 9.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-32000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-32000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-32000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-30000] due to args.save_total_limit\n",
      " 70%|██████▉   | 32500/46662 [3:41:39<1:02:38,  3.77it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1002, 'learning_rate': 1.5175088937465177e-05, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 70%|██████▉   | 32500/46662 [3:42:28<1:02:38,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-32500\n",
      "Configuration saved in ./result\\checkpoint-32500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0999478101730347, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5373, 'eval_samples_per_second': 100.934, 'eval_steps_per_second': 12.617, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-32500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-32500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-32500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-30500] due to args.save_total_limit\n",
      " 71%|███████   | 33000/46662 [3:45:04<1:00:51,  3.74it/s] ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1, 'learning_rate': 1.463932107496464e-05, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      " 71%|███████   | 33000/46662 [3:45:53<1:00:51,  3.74it/s]Saving model checkpoint to ./result\\checkpoint-33000\n",
      "Configuration saved in ./result\\checkpoint-33000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981303453445435, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5845, 'eval_samples_per_second': 100.838, 'eval_steps_per_second': 12.605, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-33000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-33000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-33000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-31000] due to args.save_total_limit\n",
      " 72%|███████▏  | 33500/46662 [3:48:28<58:05,  3.78it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.102, 'learning_rate': 1.4103553212464105e-05, 'epoch': 10.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 72%|███████▏  | 33500/46662 [3:49:18<58:05,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-33500\n",
      "Configuration saved in ./result\\checkpoint-33500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981271266937256, 'eval_accuracy': 0.3396, 'eval_runtime': 49.467, 'eval_samples_per_second': 101.078, 'eval_steps_per_second': 12.635, 'epoch': 10.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-33500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-33500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-33500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-31500] due to args.save_total_limit\n",
      " 73%|███████▎  | 34000/46662 [3:51:53<55:56,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0986, 'learning_rate': 1.356778534996357e-05, 'epoch': 10.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 73%|███████▎  | 34000/46662 [3:52:42<55:56,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-34000\n",
      "Configuration saved in ./result\\checkpoint-34000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983892679214478, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4676, 'eval_samples_per_second': 101.076, 'eval_steps_per_second': 12.635, 'epoch': 10.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-34000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-34000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-34000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-32000] due to args.save_total_limit\n",
      " 74%|███████▍  | 34500/46662 [3:55:16<53:53,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1009, 'learning_rate': 1.3032017487463033e-05, 'epoch': 10.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 74%|███████▍  | 34500/46662 [3:56:05<53:53,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-34500\n",
      "Configuration saved in ./result\\checkpoint-34500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0984256267547607, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4896, 'eval_samples_per_second': 101.031, 'eval_steps_per_second': 12.629, 'epoch': 10.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-34500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-34500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-34500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-32500] due to args.save_total_limit\n",
      " 75%|███████▌  | 35000/46662 [3:58:40<51:44,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.099, 'learning_rate': 1.2496249624962497e-05, 'epoch': 10.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 75%|███████▌  | 35000/46662 [3:59:30<51:44,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-35000\n",
      "Configuration saved in ./result\\checkpoint-35000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983635187149048, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4328, 'eval_samples_per_second': 101.147, 'eval_steps_per_second': 12.643, 'epoch': 10.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-35000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-35000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-35000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-33000] due to args.save_total_limit\n",
      " 76%|███████▌  | 35500/46662 [4:02:06<48:37,  3.83it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1022, 'learning_rate': 1.196048176246196e-05, 'epoch': 10.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 76%|███████▌  | 35500/46662 [4:02:56<48:37,  3.83it/s]Saving model checkpoint to ./result\\checkpoint-35500\n",
      "Configuration saved in ./result\\checkpoint-35500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098610281944275, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4705, 'eval_samples_per_second': 101.07, 'eval_steps_per_second': 12.634, 'epoch': 10.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-35500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-35500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-35500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-33500] due to args.save_total_limit\n",
      " 77%|███████▋  | 36000/46662 [4:05:31<47:17,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0998, 'learning_rate': 1.1424713899961425e-05, 'epoch': 10.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 77%|███████▋  | 36000/46662 [4:06:20<47:17,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-36000\n",
      "Configuration saved in ./result\\checkpoint-36000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.098675012588501, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4446, 'eval_samples_per_second': 101.123, 'eval_steps_per_second': 12.64, 'epoch': 10.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-36000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-36000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-36000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-34000] due to args.save_total_limit\n",
      " 78%|███████▊  | 36500/46662 [4:08:56<44:53,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1006, 'learning_rate': 1.088894603746089e-05, 'epoch': 10.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 78%|███████▊  | 36500/46662 [4:09:46<44:53,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-36500\n",
      "Configuration saved in ./result\\checkpoint-36500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983984470367432, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5134, 'eval_samples_per_second': 100.983, 'eval_steps_per_second': 12.623, 'epoch': 10.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-36500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-36500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-36500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-34500] due to args.save_total_limit\n",
      " 79%|███████▉  | 37000/46662 [4:12:21<42:25,  3.80it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0994, 'learning_rate': 1.0353178174960353e-05, 'epoch': 11.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 79%|███████▉  | 37000/46662 [4:13:11<42:25,  3.80it/s]Saving model checkpoint to ./result\\checkpoint-37000\n",
      "Configuration saved in ./result\\checkpoint-37000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981358289718628, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4603, 'eval_samples_per_second': 101.091, 'eval_steps_per_second': 12.636, 'epoch': 11.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-37000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-37000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-37000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-35000] due to args.save_total_limit\n",
      " 80%|████████  | 37500/46662 [4:15:47<40:30,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0996, 'learning_rate': 9.817410312459818e-06, 'epoch': 11.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 80%|████████  | 37500/46662 [4:16:36<40:30,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-37500\n",
      "Configuration saved in ./result\\checkpoint-37500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.102162480354309, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4464, 'eval_samples_per_second': 101.12, 'eval_steps_per_second': 12.64, 'epoch': 11.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-37500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-37500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-37500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-35500] due to args.save_total_limit\n",
      " 81%|████████▏ | 38000/46662 [4:19:12<38:13,  3.78it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1001, 'learning_rate': 9.281642449959283e-06, 'epoch': 11.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 81%|████████▏ | 38000/46662 [4:20:01<38:13,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-38000\n",
      "Configuration saved in ./result\\checkpoint-38000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0990060567855835, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4634, 'eval_samples_per_second': 101.085, 'eval_steps_per_second': 12.636, 'epoch': 11.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-38000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-38000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-38000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-36000] due to args.save_total_limit\n",
      " 83%|████████▎ | 38500/46662 [4:22:37<36:02,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0992, 'learning_rate': 8.745874587458746e-06, 'epoch': 11.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 83%|████████▎ | 38500/46662 [4:23:27<36:02,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-38500\n",
      "Configuration saved in ./result\\checkpoint-38500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983095169067383, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5023, 'eval_samples_per_second': 101.005, 'eval_steps_per_second': 12.626, 'epoch': 11.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-38500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-38500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-38500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-36500] due to args.save_total_limit\n",
      " 84%|████████▎ | 39000/46662 [4:26:02<34:00,  3.75it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0979, 'learning_rate': 8.210106724958211e-06, 'epoch': 11.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 84%|████████▎ | 39000/46662 [4:26:51<34:00,  3.75it/s]Saving model checkpoint to ./result\\checkpoint-39000\n",
      "Configuration saved in ./result\\checkpoint-39000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983238220214844, 'eval_accuracy': 0.3424, 'eval_runtime': 49.473, 'eval_samples_per_second': 101.065, 'eval_steps_per_second': 12.633, 'epoch': 11.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-39000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-39000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-39000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-37000] due to args.save_total_limit\n",
      " 85%|████████▍ | 39500/46662 [4:29:26<31:45,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.101, 'learning_rate': 7.674338862457676e-06, 'epoch': 11.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 85%|████████▍ | 39500/46662 [4:30:15<31:45,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-39500\n",
      "Configuration saved in ./result\\checkpoint-39500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0988701581954956, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5265, 'eval_samples_per_second': 100.956, 'eval_steps_per_second': 12.619, 'epoch': 11.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-39500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-39500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-39500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-37500] due to args.save_total_limit\n",
      " 86%|████████▌ | 40000/46662 [4:32:47<29:29,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0998, 'learning_rate': 7.13857099995714e-06, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 86%|████████▌ | 40000/46662 [4:33:37<29:29,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-40000\n",
      "Configuration saved in ./result\\checkpoint-40000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.099476933479309, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5044, 'eval_samples_per_second': 101.001, 'eval_steps_per_second': 12.625, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-40000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-38000] due to args.save_total_limit\n",
      " 87%|████████▋ | 40500/46662 [4:36:11<27:20,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0998, 'learning_rate': 6.602803137456604e-06, 'epoch': 12.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 87%|████████▋ | 40500/46662 [4:37:01<27:20,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-40500\n",
      "Configuration saved in ./result\\checkpoint-40500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981144905090332, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5469, 'eval_samples_per_second': 100.915, 'eval_steps_per_second': 12.614, 'epoch': 12.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-40500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-40500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-40500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-38500] due to args.save_total_limit\n",
      " 88%|████████▊ | 41000/46662 [4:39:36<25:07,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0996, 'learning_rate': 6.067035274956067e-06, 'epoch': 12.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|████████▊ | 41000/46662 [4:40:25<25:07,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-41000\n",
      "Configuration saved in ./result\\checkpoint-41000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.099239706993103, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5066, 'eval_samples_per_second': 100.997, 'eval_steps_per_second': 12.625, 'epoch': 12.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-41000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-41000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-41000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-39000] due to args.save_total_limit\n",
      " 89%|████████▉ | 41500/46662 [4:43:00<22:44,  3.78it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0987, 'learning_rate': 5.531267412455532e-06, 'epoch': 12.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 89%|████████▉ | 41500/46662 [4:43:50<22:44,  3.78it/s]Saving model checkpoint to ./result\\checkpoint-41500\n",
      "Configuration saved in ./result\\checkpoint-41500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0982249975204468, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5625, 'eval_samples_per_second': 100.883, 'eval_steps_per_second': 12.61, 'epoch': 12.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-41500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-41500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-41500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-39500] due to args.save_total_limit\n",
      " 90%|█████████ | 42000/46662 [4:46:26<20:40,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0984, 'learning_rate': 4.995499549954996e-06, 'epoch': 12.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 90%|█████████ | 42000/46662 [4:47:16<20:40,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-42000\n",
      "Configuration saved in ./result\\checkpoint-42000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0992110967636108, 'eval_accuracy': 0.3396, 'eval_runtime': 49.4772, 'eval_samples_per_second': 101.057, 'eval_steps_per_second': 12.632, 'epoch': 12.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-42000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-42000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-42000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-40000] due to args.save_total_limit\n",
      " 91%|█████████ | 42500/46662 [4:49:51<18:26,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.098, 'learning_rate': 4.4597316874544596e-06, 'epoch': 12.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 91%|█████████ | 42500/46662 [4:50:41<18:26,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-42500\n",
      "Configuration saved in ./result\\checkpoint-42500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0983660221099854, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5443, 'eval_samples_per_second': 100.92, 'eval_steps_per_second': 12.615, 'epoch': 12.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-42500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-42500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-42500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-40500] due to args.save_total_limit\n",
      " 92%|█████████▏| 43000/46662 [4:53:16<16:11,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0999, 'learning_rate': 3.9239638249539235e-06, 'epoch': 12.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 92%|█████████▏| 43000/46662 [4:54:06<16:11,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-43000\n",
      "Configuration saved in ./result\\checkpoint-43000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0985331535339355, 'eval_accuracy': 0.3424, 'eval_runtime': 49.4499, 'eval_samples_per_second': 101.112, 'eval_steps_per_second': 12.639, 'epoch': 12.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-43000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-43000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-43000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-41000] due to args.save_total_limit\n",
      " 93%|█████████▎| 43500/46662 [4:56:39<13:59,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0978, 'learning_rate': 3.388195962453388e-06, 'epoch': 13.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 93%|█████████▎| 43500/46662 [4:57:28<13:59,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-43500\n",
      "Configuration saved in ./result\\checkpoint-43500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981906652450562, 'eval_accuracy': 0.3396, 'eval_runtime': 49.504, 'eval_samples_per_second': 101.002, 'eval_steps_per_second': 12.625, 'epoch': 13.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-43500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-43500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-43500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-41500] due to args.save_total_limit\n",
      " 94%|█████████▍| 44000/46662 [5:00:04<11:46,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0986, 'learning_rate': 2.8524280999528527e-06, 'epoch': 13.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 94%|█████████▍| 44000/46662 [5:00:54<11:46,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-44000\n",
      "Configuration saved in ./result\\checkpoint-44000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0982004404067993, 'eval_accuracy': 0.3396, 'eval_runtime': 49.6473, 'eval_samples_per_second': 100.71, 'eval_steps_per_second': 12.589, 'epoch': 13.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-44000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-44000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-44000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-42000] due to args.save_total_limit\n",
      " 95%|█████████▌| 44500/46662 [5:03:28<09:35,  3.76it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0981, 'learning_rate': 2.3166602374523167e-06, 'epoch': 13.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 95%|█████████▌| 44500/46662 [5:04:18<09:35,  3.76it/s]Saving model checkpoint to ./result\\checkpoint-44500\n",
      "Configuration saved in ./result\\checkpoint-44500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981100797653198, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5852, 'eval_samples_per_second': 100.837, 'eval_steps_per_second': 12.605, 'epoch': 13.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-44500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-44500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-44500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-42500] due to args.save_total_limit\n",
      " 96%|█████████▋| 45000/46662 [5:06:46<07:21,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0984, 'learning_rate': 1.7808923749517811e-06, 'epoch': 13.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 96%|█████████▋| 45000/46662 [5:07:36<07:21,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-45000\n",
      "Configuration saved in ./result\\checkpoint-45000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981075763702393, 'eval_accuracy': 0.3424, 'eval_runtime': 49.6429, 'eval_samples_per_second': 100.719, 'eval_steps_per_second': 12.59, 'epoch': 13.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-45000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-45000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-45000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-43000] due to args.save_total_limit\n",
      " 98%|█████████▊| 45500/46662 [5:10:12<05:08,  3.77it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0991, 'learning_rate': 1.2451245124512453e-06, 'epoch': 13.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 98%|█████████▊| 45500/46662 [5:11:02<05:08,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-45500\n",
      "Configuration saved in ./result\\checkpoint-45500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0980815887451172, 'eval_accuracy': 0.3424, 'eval_runtime': 49.5985, 'eval_samples_per_second': 100.809, 'eval_steps_per_second': 12.601, 'epoch': 13.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-45500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-45500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-45500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-43500] due to args.save_total_limit\n",
      " 99%|█████████▊| 46000/46662 [5:13:36<02:55,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0989, 'learning_rate': 7.093566499507094e-07, 'epoch': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 99%|█████████▊| 46000/46662 [5:14:26<02:55,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-46000\n",
      "Configuration saved in ./result\\checkpoint-46000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981464385986328, 'eval_accuracy': 0.3396, 'eval_runtime': 49.626, 'eval_samples_per_second': 100.754, 'eval_steps_per_second': 12.594, 'epoch': 13.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-46000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-46000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-46000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-44000] due to args.save_total_limit\n",
      "100%|█████████▉| 46500/46662 [5:17:01<00:42,  3.77it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0997, 'learning_rate': 1.735887874501736e-07, 'epoch': 13.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|█████████▉| 46500/46662 [5:17:50<00:42,  3.77it/s]Saving model checkpoint to ./result\\checkpoint-46500\n",
      "Configuration saved in ./result\\checkpoint-46500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0981128215789795, 'eval_accuracy': 0.3396, 'eval_runtime': 49.5555, 'eval_samples_per_second': 100.897, 'eval_steps_per_second': 12.612, 'epoch': 13.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-46500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-46500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-46500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-44500] due to args.save_total_limit\n",
      "100%|██████████| 46662/46662 [5:18:55<00:00,  3.80it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./result\\checkpoint-26500 (score: 1.0980784893035889).\n",
      "100%|██████████| 46662/46662 [5:19:04<00:00,  2.44it/s]\n",
      "Configuration saved in ./result/checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 19144.5852, 'train_samples_per_second': 14.624, 'train_steps_per_second': 2.437, 'train_loss': 1.104212871739706, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result/checkpoint-4000\\pytorch_model.bin\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at C:\\Users\\bitcamp/.cache\\huggingface\\transformers\\4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at C:\\Users\\bitcamp/.cache\\huggingface\\transformers\\360b579947002f14f22331a026821b56f70679f1be1e95fe5dc5a80edc4a59e0.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at C:\\Users\\bitcamp/.cache\\huggingface\\transformers\\1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at C:\\Users\\bitcamp/.cache\\huggingface\\transformers\\8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('./result/checkpoint-4000')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Tokenizer_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result/best_model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./result/best_model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./result/best_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./result/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = './result/best_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666\n",
      "{'input_ids': tensor([    0,   720,  3994,  2052, 10428,  2775,   647,  3657,  2119,  1085,\n",
      "            3,     2,   720,  3994,  2052,   911,  2075,  3669,  2119,  3926,\n",
      "         2088,  1513,  2359, 13964,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "[CLS] 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다 [SEP] 18일 배를 타고 여행을 떠났습니다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:11<00:00,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[0, 'contradiction'], [1, 'contradiction'], [2, 'contradiction'], [3, 'contradiction'], [4, 'contradiction'], [5, 'contradiction'], [6, 'contradiction'], [7, 'contradiction'], [8, 'contradiction'], [9, 'contradiction'], [10, 'contradiction'], [11, 'contradiction'], [12, 'contradiction'], [13, 'contradiction'], [14, 'contradiction'], [15, 'contradiction'], [16, 'contradiction'], [17, 'contradiction'], [18, 'contradiction'], [19, 'contradiction'], [20, 'contradiction'], [21, 'contradiction'], [22, 'contradiction'], [23, 'contradiction'], [24, 'contradiction'], [25, 'contradiction'], [26, 'contradiction'], [27, 'contradiction'], [28, 'contradiction'], [29, 'contradiction'], [30, 'contradiction'], [31, 'contradiction'], [32, 'contradiction'], [33, 'contradiction'], [34, 'contradiction'], [35, 'contradiction'], [36, 'contradiction'], [37, 'contradiction'], [38, 'contradiction'], [39, 'contradiction'], [40, 'contradiction'], [41, 'contradiction'], [42, 'contradiction'], [43, 'contradiction'], [44, 'contradiction'], [45, 'contradiction'], [46, 'contradiction'], [47, 'contradiction'], [48, 'contradiction'], [49, 'contradiction'], [50, 'contradiction'], [51, 'contradiction'], [52, 'contradiction'], [53, 'contradiction'], [54, 'contradiction'], [55, 'contradiction'], [56, 'contradiction'], [57, 'contradiction'], [58, 'contradiction'], [59, 'contradiction'], [60, 'contradiction'], [61, 'contradiction'], [62, 'contradiction'], [63, 'contradiction'], [64, 'contradiction'], [65, 'contradiction'], [66, 'contradiction'], [67, 'contradiction'], [68, 'contradiction'], [69, 'contradiction'], [70, 'contradiction'], [71, 'contradiction'], [72, 'contradiction'], [73, 'contradiction'], [74, 'contradiction'], [75, 'contradiction'], [76, 'contradiction'], [77, 'contradiction'], [78, 'contradiction'], [79, 'contradiction'], [80, 'contradiction'], [81, 'contradiction'], [82, 'contradiction'], [83, 'contradiction'], [84, 'contradiction'], [85, 'contradiction'], [86, 'contradiction'], [87, 'contradiction'], [88, 'contradiction'], [89, 'contradiction'], [90, 'contradiction'], [91, 'contradiction'], [92, 'contradiction'], [93, 'contradiction'], [94, 'contradiction'], [95, 'contradiction'], [96, 'contradiction'], [97, 'contradiction'], [98, 'contradiction'], [99, 'contradiction'], [100, 'contradiction'], [101, 'contradiction'], [102, 'contradiction'], [103, 'contradiction'], [104, 'contradiction'], [105, 'contradiction'], [106, 'contradiction'], [107, 'contradiction'], [108, 'contradiction'], [109, 'contradiction'], [110, 'contradiction'], [111, 'contradiction'], [112, 'contradiction'], [113, 'contradiction'], [114, 'contradiction'], [115, 'contradiction'], [116, 'contradiction'], [117, 'contradiction'], [118, 'contradiction'], [119, 'contradiction'], [120, 'contradiction'], [121, 'contradiction'], [122, 'contradiction'], [123, 'contradiction'], [124, 'contradiction'], [125, 'contradiction'], [126, 'contradiction'], [127, 'contradiction'], [128, 'contradiction'], [129, 'contradiction'], [130, 'contradiction'], [131, 'contradiction'], [132, 'contradiction'], [133, 'contradiction'], [134, 'contradiction'], [135, 'contradiction'], [136, 'contradiction'], [137, 'contradiction'], [138, 'contradiction'], [139, 'contradiction'], [140, 'contradiction'], [141, 'contradiction'], [142, 'contradiction'], [143, 'contradiction'], [144, 'contradiction'], [145, 'contradiction'], [146, 'contradiction'], [147, 'contradiction'], [148, 'contradiction'], [149, 'contradiction'], [150, 'contradiction'], [151, 'contradiction'], [152, 'contradiction'], [153, 'contradiction'], [154, 'contradiction'], [155, 'contradiction'], [156, 'contradiction'], [157, 'contradiction'], [158, 'contradiction'], [159, 'contradiction'], [160, 'contradiction'], [161, 'contradiction'], [162, 'contradiction'], [163, 'contradiction'], [164, 'contradiction'], [165, 'contradiction'], [166, 'contradiction'], [167, 'contradiction'], [168, 'contradiction'], [169, 'contradiction'], [170, 'contradiction'], [171, 'contradiction'], [172, 'contradiction'], [173, 'contradiction'], [174, 'contradiction'], [175, 'contradiction'], [176, 'contradiction'], [177, 'contradiction'], [178, 'contradiction'], [179, 'contradiction'], [180, 'contradiction'], [181, 'contradiction'], [182, 'contradiction'], [183, 'contradiction'], [184, 'contradiction'], [185, 'contradiction'], [186, 'contradiction'], [187, 'contradiction'], [188, 'contradiction'], [189, 'contradiction'], [190, 'contradiction'], [191, 'contradiction'], [192, 'contradiction'], [193, 'contradiction'], [194, 'contradiction'], [195, 'contradiction'], [196, 'contradiction'], [197, 'contradiction'], [198, 'contradiction'], [199, 'contradiction'], [200, 'contradiction'], [201, 'contradiction'], [202, 'contradiction'], [203, 'contradiction'], [204, 'contradiction'], [205, 'contradiction'], [206, 'contradiction'], [207, 'contradiction'], [208, 'contradiction'], [209, 'contradiction'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'contradiction'], [214, 'contradiction'], [215, 'contradiction'], [216, 'contradiction'], [217, 'contradiction'], [218, 'contradiction'], [219, 'contradiction'], [220, 'contradiction'], [221, 'contradiction'], [222, 'contradiction'], [223, 'contradiction'], [224, 'contradiction'], [225, 'contradiction'], [226, 'contradiction'], [227, 'contradiction'], [228, 'contradiction'], [229, 'contradiction'], [230, 'contradiction'], [231, 'contradiction'], [232, 'contradiction'], [233, 'contradiction'], [234, 'contradiction'], [235, 'contradiction'], [236, 'contradiction'], [237, 'contradiction'], [238, 'contradiction'], [239, 'contradiction'], [240, 'contradiction'], [241, 'contradiction'], [242, 'contradiction'], [243, 'contradiction'], [244, 'contradiction'], [245, 'contradiction'], [246, 'contradiction'], [247, 'contradiction'], [248, 'contradiction'], [249, 'contradiction'], [250, 'contradiction'], [251, 'contradiction'], [252, 'contradiction'], [253, 'contradiction'], [254, 'contradiction'], [255, 'contradiction'], [256, 'contradiction'], [257, 'contradiction'], [258, 'contradiction'], [259, 'contradiction'], [260, 'contradiction'], [261, 'contradiction'], [262, 'contradiction'], [263, 'contradiction'], [264, 'contradiction'], [265, 'contradiction'], [266, 'contradiction'], [267, 'contradiction'], [268, 'contradiction'], [269, 'contradiction'], [270, 'contradiction'], [271, 'contradiction'], [272, 'contradiction'], [273, 'contradiction'], [274, 'contradiction'], [275, 'contradiction'], [276, 'contradiction'], [277, 'contradiction'], [278, 'contradiction'], [279, 'contradiction'], [280, 'contradiction'], [281, 'contradiction'], [282, 'contradiction'], [283, 'contradiction'], [284, 'contradiction'], [285, 'contradiction'], [286, 'contradiction'], [287, 'contradiction'], [288, 'contradiction'], [289, 'contradiction'], [290, 'contradiction'], [291, 'contradiction'], [292, 'contradiction'], [293, 'contradiction'], [294, 'contradiction'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'contradiction'], [299, 'contradiction'], [300, 'contradiction'], [301, 'contradiction'], [302, 'contradiction'], [303, 'contradiction'], [304, 'contradiction'], [305, 'contradiction'], [306, 'contradiction'], [307, 'contradiction'], [308, 'contradiction'], [309, 'contradiction'], [310, 'contradiction'], [311, 'contradiction'], [312, 'contradiction'], [313, 'contradiction'], [314, 'contradiction'], [315, 'contradiction'], [316, 'contradiction'], [317, 'contradiction'], [318, 'contradiction'], [319, 'contradiction'], [320, 'contradiction'], [321, 'contradiction'], [322, 'contradiction'], [323, 'contradiction'], [324, 'contradiction'], [325, 'contradiction'], [326, 'contradiction'], [327, 'contradiction'], [328, 'contradiction'], [329, 'contradiction'], [330, 'contradiction'], [331, 'contradiction'], [332, 'contradiction'], [333, 'contradiction'], [334, 'contradiction'], [335, 'contradiction'], [336, 'contradiction'], [337, 'contradiction'], [338, 'contradiction'], [339, 'contradiction'], [340, 'contradiction'], [341, 'contradiction'], [342, 'contradiction'], [343, 'contradiction'], [344, 'contradiction'], [345, 'contradiction'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'contradiction'], [350, 'contradiction'], [351, 'contradiction'], [352, 'contradiction'], [353, 'contradiction'], [354, 'contradiction'], [355, 'contradiction'], [356, 'contradiction'], [357, 'contradiction'], [358, 'contradiction'], [359, 'contradiction'], [360, 'contradiction'], [361, 'contradiction'], [362, 'contradiction'], [363, 'contradiction'], [364, 'contradiction'], [365, 'contradiction'], [366, 'contradiction'], [367, 'contradiction'], [368, 'contradiction'], [369, 'contradiction'], [370, 'contradiction'], [371, 'contradiction'], [372, 'contradiction'], [373, 'contradiction'], [374, 'contradiction'], [375, 'contradiction'], [376, 'contradiction'], [377, 'contradiction'], [378, 'contradiction'], [379, 'contradiction'], [380, 'contradiction'], [381, 'contradiction'], [382, 'contradiction'], [383, 'contradiction'], [384, 'contradiction'], [385, 'contradiction'], [386, 'contradiction'], [387, 'contradiction'], [388, 'contradiction'], [389, 'contradiction'], [390, 'contradiction'], [391, 'contradiction'], [392, 'contradiction'], [393, 'contradiction'], [394, 'contradiction'], [395, 'contradiction'], [396, 'contradiction'], [397, 'contradiction'], [398, 'contradiction'], [399, 'contradiction'], [400, 'contradiction'], [401, 'contradiction'], [402, 'contradiction'], [403, 'contradiction'], [404, 'contradiction'], [405, 'contradiction'], [406, 'contradiction'], [407, 'contradiction'], [408, 'contradiction'], [409, 'contradiction'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'contradiction'], [415, 'contradiction'], [416, 'contradiction'], [417, 'contradiction'], [418, 'contradiction'], [419, 'contradiction'], [420, 'contradiction'], [421, 'contradiction'], [422, 'contradiction'], [423, 'contradiction'], [424, 'contradiction'], [425, 'contradiction'], [426, 'contradiction'], [427, 'contradiction'], [428, 'contradiction'], [429, 'contradiction'], [430, 'contradiction'], [431, 'contradiction'], [432, 'contradiction'], [433, 'contradiction'], [434, 'contradiction'], [435, 'contradiction'], [436, 'contradiction'], [437, 'contradiction'], [438, 'contradiction'], [439, 'contradiction'], [440, 'contradiction'], [441, 'contradiction'], [442, 'contradiction'], [443, 'contradiction'], [444, 'contradiction'], [445, 'contradiction'], [446, 'contradiction'], [447, 'contradiction'], [448, 'contradiction'], [449, 'contradiction'], [450, 'contradiction'], [451, 'contradiction'], [452, 'contradiction'], [453, 'contradiction'], [454, 'contradiction'], [455, 'contradiction'], [456, 'contradiction'], [457, 'contradiction'], [458, 'contradiction'], [459, 'contradiction'], [460, 'contradiction'], [461, 'contradiction'], [462, 'contradiction'], [463, 'contradiction'], [464, 'contradiction'], [465, 'contradiction'], [466, 'contradiction'], [467, 'contradiction'], [468, 'contradiction'], [469, 'contradiction'], [470, 'contradiction'], [471, 'contradiction'], [472, 'contradiction'], [473, 'contradiction'], [474, 'contradiction'], [475, 'contradiction'], [476, 'contradiction'], [477, 'contradiction'], [478, 'contradiction'], [479, 'contradiction'], [480, 'contradiction'], [481, 'contradiction'], [482, 'contradiction'], [483, 'contradiction'], [484, 'contradiction'], [485, 'contradiction'], [486, 'contradiction'], [487, 'contradiction'], [488, 'contradiction'], [489, 'contradiction'], [490, 'contradiction'], [491, 'contradiction'], [492, 'contradiction'], [493, 'contradiction'], [494, 'contradiction'], [495, 'contradiction'], [496, 'contradiction'], [497, 'contradiction'], [498, 'contradiction'], [499, 'contradiction'], [500, 'contradiction'], [501, 'contradiction'], [502, 'contradiction'], [503, 'contradiction'], [504, 'contradiction'], [505, 'contradiction'], [506, 'contradiction'], [507, 'contradiction'], [508, 'contradiction'], [509, 'contradiction'], [510, 'contradiction'], [511, 'contradiction'], [512, 'contradiction'], [513, 'contradiction'], [514, 'contradiction'], [515, 'contradiction'], [516, 'contradiction'], [517, 'contradiction'], [518, 'contradiction'], [519, 'contradiction'], [520, 'contradiction'], [521, 'contradiction'], [522, 'contradiction'], [523, 'contradiction'], [524, 'contradiction'], [525, 'contradiction'], [526, 'contradiction'], [527, 'contradiction'], [528, 'contradiction'], [529, 'contradiction'], [530, 'contradiction'], [531, 'contradiction'], [532, 'contradiction'], [533, 'contradiction'], [534, 'contradiction'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'contradiction'], [539, 'contradiction'], [540, 'contradiction'], [541, 'contradiction'], [542, 'contradiction'], [543, 'contradiction'], [544, 'contradiction'], [545, 'contradiction'], [546, 'contradiction'], [547, 'contradiction'], [548, 'contradiction'], [549, 'contradiction'], [550, 'contradiction'], [551, 'contradiction'], [552, 'contradiction'], [553, 'contradiction'], [554, 'contradiction'], [555, 'contradiction'], [556, 'contradiction'], [557, 'contradiction'], [558, 'contradiction'], [559, 'contradiction'], [560, 'contradiction'], [561, 'contradiction'], [562, 'contradiction'], [563, 'contradiction'], [564, 'contradiction'], [565, 'contradiction'], [566, 'contradiction'], [567, 'contradiction'], [568, 'contradiction'], [569, 'contradiction'], [570, 'contradiction'], [571, 'contradiction'], [572, 'contradiction'], [573, 'contradiction'], [574, 'contradiction'], [575, 'contradiction'], [576, 'contradiction'], [577, 'contradiction'], [578, 'contradiction'], [579, 'contradiction'], [580, 'contradiction'], [581, 'contradiction'], [582, 'contradiction'], [583, 'contradiction'], [584, 'contradiction'], [585, 'contradiction'], [586, 'contradiction'], [587, 'contradiction'], [588, 'contradiction'], [589, 'contradiction'], [590, 'contradiction'], [591, 'contradiction'], [592, 'contradiction'], [593, 'contradiction'], [594, 'contradiction'], [595, 'contradiction'], [596, 'contradiction'], [597, 'contradiction'], [598, 'contradiction'], [599, 'contradiction'], [600, 'contradiction'], [601, 'contradiction'], [602, 'contradiction'], [603, 'contradiction'], [604, 'contradiction'], [605, 'contradiction'], [606, 'contradiction'], [607, 'contradiction'], [608, 'contradiction'], [609, 'contradiction'], [610, 'contradiction'], [611, 'contradiction'], [612, 'contradiction'], [613, 'contradiction'], [614, 'contradiction'], [615, 'contradiction'], [616, 'contradiction'], [617, 'contradiction'], [618, 'contradiction'], [619, 'contradiction'], [620, 'contradiction'], [621, 'contradiction'], [622, 'contradiction'], [623, 'contradiction'], [624, 'contradiction'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'contradiction'], [629, 'contradiction'], [630, 'contradiction'], [631, 'contradiction'], [632, 'contradiction'], [633, 'contradiction'], [634, 'contradiction'], [635, 'contradiction'], [636, 'contradiction'], [637, 'contradiction'], [638, 'contradiction'], [639, 'contradiction'], [640, 'contradiction'], [641, 'contradiction'], [642, 'contradiction'], [643, 'contradiction'], [644, 'contradiction'], [645, 'contradiction'], [646, 'contradiction'], [647, 'contradiction'], [648, 'contradiction'], [649, 'contradiction'], [650, 'contradiction'], [651, 'contradiction'], [652, 'contradiction'], [653, 'contradiction'], [654, 'contradiction'], [655, 'contradiction'], [656, 'contradiction'], [657, 'contradiction'], [658, 'contradiction'], [659, 'contradiction'], [660, 'contradiction'], [661, 'contradiction'], [662, 'contradiction'], [663, 'contradiction'], [664, 'contradiction'], [665, 'contradiction'], [666, 'contradiction'], [667, 'contradiction'], [668, 'contradiction'], [669, 'contradiction'], [670, 'contradiction'], [671, 'contradiction'], [672, 'contradiction'], [673, 'contradiction'], [674, 'contradiction'], [675, 'contradiction'], [676, 'contradiction'], [677, 'contradiction'], [678, 'contradiction'], [679, 'contradiction'], [680, 'contradiction'], [681, 'contradiction'], [682, 'contradiction'], [683, 'contradiction'], [684, 'contradiction'], [685, 'contradiction'], [686, 'contradiction'], [687, 'contradiction'], [688, 'contradiction'], [689, 'contradiction'], [690, 'contradiction'], [691, 'contradiction'], [692, 'contradiction'], [693, 'contradiction'], [694, 'contradiction'], [695, 'contradiction'], [696, 'contradiction'], [697, 'contradiction'], [698, 'contradiction'], [699, 'contradiction'], [700, 'contradiction'], [701, 'contradiction'], [702, 'contradiction'], [703, 'contradiction'], [704, 'contradiction'], [705, 'contradiction'], [706, 'contradiction'], [707, 'contradiction'], [708, 'contradiction'], [709, 'contradiction'], [710, 'contradiction'], [711, 'contradiction'], [712, 'contradiction'], [713, 'contradiction'], [714, 'contradiction'], [715, 'contradiction'], [716, 'contradiction'], [717, 'contradiction'], [718, 'contradiction'], [719, 'contradiction'], [720, 'contradiction'], [721, 'contradiction'], [722, 'contradiction'], [723, 'contradiction'], [724, 'contradiction'], [725, 'contradiction'], [726, 'contradiction'], [727, 'contradiction'], [728, 'contradiction'], [729, 'contradiction'], [730, 'contradiction'], [731, 'contradiction'], [732, 'contradiction'], [733, 'contradiction'], [734, 'contradiction'], [735, 'contradiction'], [736, 'contradiction'], [737, 'contradiction'], [738, 'contradiction'], [739, 'contradiction'], [740, 'contradiction'], [741, 'contradiction'], [742, 'contradiction'], [743, 'contradiction'], [744, 'contradiction'], [745, 'contradiction'], [746, 'contradiction'], [747, 'contradiction'], [748, 'contradiction'], [749, 'contradiction'], [750, 'contradiction'], [751, 'contradiction'], [752, 'contradiction'], [753, 'contradiction'], [754, 'contradiction'], [755, 'contradiction'], [756, 'contradiction'], [757, 'contradiction'], [758, 'contradiction'], [759, 'contradiction'], [760, 'contradiction'], [761, 'contradiction'], [762, 'contradiction'], [763, 'contradiction'], [764, 'contradiction'], [765, 'contradiction'], [766, 'contradiction'], [767, 'contradiction'], [768, 'contradiction'], [769, 'contradiction'], [770, 'contradiction'], [771, 'contradiction'], [772, 'contradiction'], [773, 'contradiction'], [774, 'contradiction'], [775, 'contradiction'], [776, 'contradiction'], [777, 'contradiction'], [778, 'contradiction'], [779, 'contradiction'], [780, 'contradiction'], [781, 'contradiction'], [782, 'contradiction'], [783, 'contradiction'], [784, 'contradiction'], [785, 'contradiction'], [786, 'contradiction'], [787, 'contradiction'], [788, 'contradiction'], [789, 'contradiction'], [790, 'contradiction'], [791, 'contradiction'], [792, 'contradiction'], [793, 'contradiction'], [794, 'contradiction'], [795, 'contradiction'], [796, 'contradiction'], [797, 'contradiction'], [798, 'contradiction'], [799, 'contradiction'], [800, 'contradiction'], [801, 'contradiction'], [802, 'contradiction'], [803, 'contradiction'], [804, 'contradiction'], [805, 'contradiction'], [806, 'contradiction'], [807, 'contradiction'], [808, 'contradiction'], [809, 'contradiction'], [810, 'contradiction'], [811, 'contradiction'], [812, 'contradiction'], [813, 'contradiction'], [814, 'contradiction'], [815, 'contradiction'], [816, 'contradiction'], [817, 'contradiction'], [818, 'contradiction'], [819, 'contradiction'], [820, 'contradiction'], [821, 'contradiction'], [822, 'contradiction'], [823, 'contradiction'], [824, 'contradiction'], [825, 'contradiction'], [826, 'contradiction'], [827, 'contradiction'], [828, 'contradiction'], [829, 'contradiction'], [830, 'contradiction'], [831, 'contradiction'], [832, 'contradiction'], [833, 'contradiction'], [834, 'contradiction'], [835, 'contradiction'], [836, 'contradiction'], [837, 'contradiction'], [838, 'contradiction'], [839, 'contradiction'], [840, 'contradiction'], [841, 'contradiction'], [842, 'contradiction'], [843, 'contradiction'], [844, 'contradiction'], [845, 'contradiction'], [846, 'contradiction'], [847, 'contradiction'], [848, 'contradiction'], [849, 'contradiction'], [850, 'contradiction'], [851, 'contradiction'], [852, 'contradiction'], [853, 'contradiction'], [854, 'contradiction'], [855, 'contradiction'], [856, 'contradiction'], [857, 'contradiction'], [858, 'contradiction'], [859, 'contradiction'], [860, 'contradiction'], [861, 'contradiction'], [862, 'contradiction'], [863, 'contradiction'], [864, 'contradiction'], [865, 'contradiction'], [866, 'contradiction'], [867, 'contradiction'], [868, 'contradiction'], [869, 'contradiction'], [870, 'contradiction'], [871, 'contradiction'], [872, 'contradiction'], [873, 'contradiction'], [874, 'contradiction'], [875, 'contradiction'], [876, 'contradiction'], [877, 'contradiction'], [878, 'contradiction'], [879, 'contradiction'], [880, 'contradiction'], [881, 'contradiction'], [882, 'contradiction'], [883, 'contradiction'], [884, 'contradiction'], [885, 'contradiction'], [886, 'contradiction'], [887, 'contradiction'], [888, 'contradiction'], [889, 'contradiction'], [890, 'contradiction'], [891, 'contradiction'], [892, 'contradiction'], [893, 'contradiction'], [894, 'contradiction'], [895, 'contradiction'], [896, 'contradiction'], [897, 'contradiction'], [898, 'contradiction'], [899, 'contradiction'], [900, 'contradiction'], [901, 'contradiction'], [902, 'contradiction'], [903, 'contradiction'], [904, 'contradiction'], [905, 'contradiction'], [906, 'contradiction'], [907, 'contradiction'], [908, 'contradiction'], [909, 'contradiction'], [910, 'contradiction'], [911, 'contradiction'], [912, 'contradiction'], [913, 'contradiction'], [914, 'contradiction'], [915, 'contradiction'], [916, 'contradiction'], [917, 'contradiction'], [918, 'contradiction'], [919, 'contradiction'], [920, 'contradiction'], [921, 'contradiction'], [922, 'contradiction'], [923, 'contradiction'], [924, 'contradiction'], [925, 'contradiction'], [926, 'contradiction'], [927, 'contradiction'], [928, 'contradiction'], [929, 'contradiction'], [930, 'contradiction'], [931, 'contradiction'], [932, 'contradiction'], [933, 'contradiction'], [934, 'contradiction'], [935, 'contradiction'], [936, 'contradiction'], [937, 'contradiction'], [938, 'contradiction'], [939, 'contradiction'], [940, 'contradiction'], [941, 'contradiction'], [942, 'contradiction'], [943, 'contradiction'], [944, 'contradiction'], [945, 'contradiction'], [946, 'contradiction'], [947, 'contradiction'], [948, 'contradiction'], [949, 'contradiction'], [950, 'contradiction'], [951, 'contradiction'], [952, 'contradiction'], [953, 'contradiction'], [954, 'contradiction'], [955, 'contradiction'], [956, 'contradiction'], [957, 'contradiction'], [958, 'contradiction'], [959, 'contradiction'], [960, 'contradiction'], [961, 'contradiction'], [962, 'contradiction'], [963, 'contradiction'], [964, 'contradiction'], [965, 'contradiction'], [966, 'contradiction'], [967, 'contradiction'], [968, 'contradiction'], [969, 'contradiction'], [970, 'contradiction'], [971, 'contradiction'], [972, 'contradiction'], [973, 'contradiction'], [974, 'contradiction'], [975, 'contradiction'], [976, 'contradiction'], [977, 'contradiction'], [978, 'contradiction'], [979, 'contradiction'], [980, 'contradiction'], [981, 'contradiction'], [982, 'contradiction'], [983, 'contradiction'], [984, 'contradiction'], [985, 'contradiction'], [986, 'contradiction'], [987, 'contradiction'], [988, 'contradiction'], [989, 'contradiction'], [990, 'contradiction'], [991, 'contradiction'], [992, 'contradiction'], [993, 'contradiction'], [994, 'contradiction'], [995, 'contradiction'], [996, 'contradiction'], [997, 'contradiction'], [998, 'contradiction'], [999, 'contradiction'], [1000, 'contradiction'], [1001, 'contradiction'], [1002, 'contradiction'], [1003, 'contradiction'], [1004, 'contradiction'], [1005, 'contradiction'], [1006, 'contradiction'], [1007, 'contradiction'], [1008, 'contradiction'], [1009, 'contradiction'], [1010, 'contradiction'], [1011, 'contradiction'], [1012, 'contradiction'], [1013, 'contradiction'], [1014, 'contradiction'], [1015, 'contradiction'], [1016, 'contradiction'], [1017, 'contradiction'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'contradiction'], [1021, 'contradiction'], [1022, 'contradiction'], [1023, 'contradiction'], [1024, 'contradiction'], [1025, 'contradiction'], [1026, 'contradiction'], [1027, 'contradiction'], [1028, 'contradiction'], [1029, 'contradiction'], [1030, 'contradiction'], [1031, 'contradiction'], [1032, 'contradiction'], [1033, 'contradiction'], [1034, 'contradiction'], [1035, 'contradiction'], [1036, 'contradiction'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'contradiction'], [1041, 'contradiction'], [1042, 'contradiction'], [1043, 'contradiction'], [1044, 'contradiction'], [1045, 'contradiction'], [1046, 'contradiction'], [1047, 'contradiction'], [1048, 'contradiction'], [1049, 'contradiction'], [1050, 'contradiction'], [1051, 'contradiction'], [1052, 'contradiction'], [1053, 'contradiction'], [1054, 'contradiction'], [1055, 'contradiction'], [1056, 'contradiction'], [1057, 'contradiction'], [1058, 'contradiction'], [1059, 'contradiction'], [1060, 'contradiction'], [1061, 'contradiction'], [1062, 'contradiction'], [1063, 'contradiction'], [1064, 'contradiction'], [1065, 'contradiction'], [1066, 'contradiction'], [1067, 'contradiction'], [1068, 'contradiction'], [1069, 'contradiction'], [1070, 'contradiction'], [1071, 'contradiction'], [1072, 'contradiction'], [1073, 'contradiction'], [1074, 'contradiction'], [1075, 'contradiction'], [1076, 'contradiction'], [1077, 'contradiction'], [1078, 'contradiction'], [1079, 'contradiction'], [1080, 'contradiction'], [1081, 'contradiction'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'contradiction'], [1085, 'contradiction'], [1086, 'contradiction'], [1087, 'contradiction'], [1088, 'contradiction'], [1089, 'contradiction'], [1090, 'contradiction'], [1091, 'contradiction'], [1092, 'contradiction'], [1093, 'contradiction'], [1094, 'contradiction'], [1095, 'contradiction'], [1096, 'contradiction'], [1097, 'contradiction'], [1098, 'contradiction'], [1099, 'contradiction'], [1100, 'contradiction'], [1101, 'contradiction'], [1102, 'contradiction'], [1103, 'contradiction'], [1104, 'contradiction'], [1105, 'contradiction'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'contradiction'], [1109, 'contradiction'], [1110, 'contradiction'], [1111, 'contradiction'], [1112, 'contradiction'], [1113, 'contradiction'], [1114, 'contradiction'], [1115, 'contradiction'], [1116, 'contradiction'], [1117, 'contradiction'], [1118, 'contradiction'], [1119, 'contradiction'], [1120, 'contradiction'], [1121, 'contradiction'], [1122, 'contradiction'], [1123, 'contradiction'], [1124, 'contradiction'], [1125, 'contradiction'], [1126, 'contradiction'], [1127, 'contradiction'], [1128, 'contradiction'], [1129, 'contradiction'], [1130, 'contradiction'], [1131, 'contradiction'], [1132, 'contradiction'], [1133, 'contradiction'], [1134, 'contradiction'], [1135, 'contradiction'], [1136, 'contradiction'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'contradiction'], [1140, 'contradiction'], [1141, 'contradiction'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'contradiction'], [1145, 'contradiction'], [1146, 'contradiction'], [1147, 'contradiction'], [1148, 'contradiction'], [1149, 'contradiction'], [1150, 'contradiction'], [1151, 'contradiction'], [1152, 'contradiction'], [1153, 'contradiction'], [1154, 'contradiction'], [1155, 'contradiction'], [1156, 'contradiction'], [1157, 'contradiction'], [1158, 'contradiction'], [1159, 'contradiction'], [1160, 'contradiction'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'contradiction'], [1164, 'contradiction'], [1165, 'contradiction'], [1166, 'contradiction'], [1167, 'contradiction'], [1168, 'contradiction'], [1169, 'contradiction'], [1170, 'contradiction'], [1171, 'contradiction'], [1172, 'contradiction'], [1173, 'contradiction'], [1174, 'contradiction'], [1175, 'contradiction'], [1176, 'contradiction'], [1177, 'contradiction'], [1178, 'contradiction'], [1179, 'contradiction'], [1180, 'contradiction'], [1181, 'contradiction'], [1182, 'contradiction'], [1183, 'contradiction'], [1184, 'contradiction'], [1185, 'contradiction'], [1186, 'contradiction'], [1187, 'contradiction'], [1188, 'contradiction'], [1189, 'contradiction'], [1190, 'contradiction'], [1191, 'contradiction'], [1192, 'contradiction'], [1193, 'contradiction'], [1194, 'contradiction'], [1195, 'contradiction'], [1196, 'contradiction'], [1197, 'contradiction'], [1198, 'contradiction'], [1199, 'contradiction'], [1200, 'contradiction'], [1201, 'contradiction'], [1202, 'contradiction'], [1203, 'contradiction'], [1204, 'contradiction'], [1205, 'contradiction'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'contradiction'], [1209, 'contradiction'], [1210, 'contradiction'], [1211, 'contradiction'], [1212, 'contradiction'], [1213, 'contradiction'], [1214, 'contradiction'], [1215, 'contradiction'], [1216, 'contradiction'], [1217, 'contradiction'], [1218, 'contradiction'], [1219, 'contradiction'], [1220, 'contradiction'], [1221, 'contradiction'], [1222, 'contradiction'], [1223, 'contradiction'], [1224, 'contradiction'], [1225, 'contradiction'], [1226, 'contradiction'], [1227, 'contradiction'], [1228, 'contradiction'], [1229, 'contradiction'], [1230, 'contradiction'], [1231, 'contradiction'], [1232, 'contradiction'], [1233, 'contradiction'], [1234, 'contradiction'], [1235, 'contradiction'], [1236, 'contradiction'], [1237, 'contradiction'], [1238, 'contradiction'], [1239, 'contradiction'], [1240, 'contradiction'], [1241, 'contradiction'], [1242, 'contradiction'], [1243, 'contradiction'], [1244, 'contradiction'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'contradiction'], [1248, 'contradiction'], [1249, 'contradiction'], [1250, 'contradiction'], [1251, 'contradiction'], [1252, 'contradiction'], [1253, 'contradiction'], [1254, 'contradiction'], [1255, 'contradiction'], [1256, 'contradiction'], [1257, 'contradiction'], [1258, 'contradiction'], [1259, 'contradiction'], [1260, 'contradiction'], [1261, 'contradiction'], [1262, 'contradiction'], [1263, 'contradiction'], [1264, 'contradiction'], [1265, 'contradiction'], [1266, 'contradiction'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'contradiction'], [1271, 'contradiction'], [1272, 'contradiction'], [1273, 'contradiction'], [1274, 'contradiction'], [1275, 'contradiction'], [1276, 'contradiction'], [1277, 'contradiction'], [1278, 'contradiction'], [1279, 'contradiction'], [1280, 'contradiction'], [1281, 'contradiction'], [1282, 'contradiction'], [1283, 'contradiction'], [1284, 'contradiction'], [1285, 'contradiction'], [1286, 'contradiction'], [1287, 'contradiction'], [1288, 'contradiction'], [1289, 'contradiction'], [1290, 'contradiction'], [1291, 'contradiction'], [1292, 'contradiction'], [1293, 'contradiction'], [1294, 'contradiction'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'contradiction'], [1298, 'contradiction'], [1299, 'contradiction'], [1300, 'contradiction'], [1301, 'contradiction'], [1302, 'contradiction'], [1303, 'contradiction'], [1304, 'contradiction'], [1305, 'contradiction'], [1306, 'contradiction'], [1307, 'contradiction'], [1308, 'contradiction'], [1309, 'contradiction'], [1310, 'contradiction'], [1311, 'contradiction'], [1312, 'contradiction'], [1313, 'contradiction'], [1314, 'contradiction'], [1315, 'contradiction'], [1316, 'contradiction'], [1317, 'contradiction'], [1318, 'contradiction'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'contradiction'], [1322, 'contradiction'], [1323, 'contradiction'], [1324, 'contradiction'], [1325, 'contradiction'], [1326, 'contradiction'], [1327, 'contradiction'], [1328, 'contradiction'], [1329, 'contradiction'], [1330, 'contradiction'], [1331, 'contradiction'], [1332, 'contradiction'], [1333, 'contradiction'], [1334, 'contradiction'], [1335, 'contradiction'], [1336, 'contradiction'], [1337, 'contradiction'], [1338, 'contradiction'], [1339, 'contradiction'], [1340, 'contradiction'], [1341, 'contradiction'], [1342, 'contradiction'], [1343, 'contradiction'], [1344, 'contradiction'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'contradiction'], [1348, 'contradiction'], [1349, 'contradiction'], [1350, 'contradiction'], [1351, 'contradiction'], [1352, 'contradiction'], [1353, 'contradiction'], [1354, 'contradiction'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'contradiction'], [1359, 'contradiction'], [1360, 'contradiction'], [1361, 'contradiction'], [1362, 'contradiction'], [1363, 'contradiction'], [1364, 'contradiction'], [1365, 'contradiction'], [1366, 'contradiction'], [1367, 'contradiction'], [1368, 'contradiction'], [1369, 'contradiction'], [1370, 'contradiction'], [1371, 'contradiction'], [1372, 'contradiction'], [1373, 'contradiction'], [1374, 'contradiction'], [1375, 'contradiction'], [1376, 'contradiction'], [1377, 'contradiction'], [1378, 'contradiction'], [1379, 'contradiction'], [1380, 'contradiction'], [1381, 'contradiction'], [1382, 'contradiction'], [1383, 'contradiction'], [1384, 'contradiction'], [1385, 'contradiction'], [1386, 'contradiction'], [1387, 'contradiction'], [1388, 'contradiction'], [1389, 'contradiction'], [1390, 'contradiction'], [1391, 'contradiction'], [1392, 'contradiction'], [1393, 'contradiction'], [1394, 'contradiction'], [1395, 'contradiction'], [1396, 'contradiction'], [1397, 'contradiction'], [1398, 'contradiction'], [1399, 'contradiction'], [1400, 'contradiction'], [1401, 'contradiction'], [1402, 'contradiction'], [1403, 'contradiction'], [1404, 'contradiction'], [1405, 'contradiction'], [1406, 'contradiction'], [1407, 'contradiction'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'contradiction'], [1411, 'contradiction'], [1412, 'contradiction'], [1413, 'contradiction'], [1414, 'contradiction'], [1415, 'contradiction'], [1416, 'contradiction'], [1417, 'contradiction'], [1418, 'contradiction'], [1419, 'contradiction'], [1420, 'contradiction'], [1421, 'contradiction'], [1422, 'contradiction'], [1423, 'contradiction'], [1424, 'contradiction'], [1425, 'contradiction'], [1426, 'contradiction'], [1427, 'contradiction'], [1428, 'contradiction'], [1429, 'contradiction'], [1430, 'contradiction'], [1431, 'contradiction'], [1432, 'contradiction'], [1433, 'contradiction'], [1434, 'contradiction'], [1435, 'contradiction'], [1436, 'contradiction'], [1437, 'contradiction'], [1438, 'contradiction'], [1439, 'contradiction'], [1440, 'contradiction'], [1441, 'contradiction'], [1442, 'contradiction'], [1443, 'contradiction'], [1444, 'contradiction'], [1445, 'contradiction'], [1446, 'contradiction'], [1447, 'contradiction'], [1448, 'contradiction'], [1449, 'contradiction'], [1450, 'contradiction'], [1451, 'contradiction'], [1452, 'contradiction'], [1453, 'contradiction'], [1454, 'contradiction'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'contradiction'], [1458, 'contradiction'], [1459, 'contradiction'], [1460, 'contradiction'], [1461, 'contradiction'], [1462, 'contradiction'], [1463, 'contradiction'], [1464, 'contradiction'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'contradiction'], [1468, 'contradiction'], [1469, 'contradiction'], [1470, 'contradiction'], [1471, 'contradiction'], [1472, 'contradiction'], [1473, 'contradiction'], [1474, 'contradiction'], [1475, 'contradiction'], [1476, 'contradiction'], [1477, 'contradiction'], [1478, 'contradiction'], [1479, 'contradiction'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'contradiction'], [1484, 'contradiction'], [1485, 'contradiction'], [1486, 'contradiction'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'contradiction'], [1490, 'contradiction'], [1491, 'contradiction'], [1492, 'contradiction'], [1493, 'contradiction'], [1494, 'contradiction'], [1495, 'contradiction'], [1496, 'contradiction'], [1497, 'contradiction'], [1498, 'contradiction'], [1499, 'contradiction'], [1500, 'contradiction'], [1501, 'contradiction'], [1502, 'contradiction'], [1503, 'contradiction'], [1504, 'contradiction'], [1505, 'contradiction'], [1506, 'contradiction'], [1507, 'contradiction'], [1508, 'contradiction'], [1509, 'contradiction'], [1510, 'contradiction'], [1511, 'contradiction'], [1512, 'contradiction'], [1513, 'contradiction'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'contradiction'], [1517, 'contradiction'], [1518, 'contradiction'], [1519, 'contradiction'], [1520, 'contradiction'], [1521, 'contradiction'], [1522, 'contradiction'], [1523, 'contradiction'], [1524, 'contradiction'], [1525, 'contradiction'], [1526, 'contradiction'], [1527, 'contradiction'], [1528, 'contradiction'], [1529, 'contradiction'], [1530, 'contradiction'], [1531, 'contradiction'], [1532, 'contradiction'], [1533, 'contradiction'], [1534, 'contradiction'], [1535, 'contradiction'], [1536, 'contradiction'], [1537, 'contradiction'], [1538, 'contradiction'], [1539, 'contradiction'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'contradiction'], [1543, 'contradiction'], [1544, 'contradiction'], [1545, 'contradiction'], [1546, 'contradiction'], [1547, 'contradiction'], [1548, 'contradiction'], [1549, 'contradiction'], [1550, 'contradiction'], [1551, 'contradiction'], [1552, 'contradiction'], [1553, 'contradiction'], [1554, 'contradiction'], [1555, 'contradiction'], [1556, 'contradiction'], [1557, 'contradiction'], [1558, 'contradiction'], [1559, 'contradiction'], [1560, 'contradiction'], [1561, 'contradiction'], [1562, 'contradiction'], [1563, 'contradiction'], [1564, 'contradiction'], [1565, 'contradiction'], [1566, 'contradiction'], [1567, 'contradiction'], [1568, 'contradiction'], [1569, 'contradiction'], [1570, 'contradiction'], [1571, 'contradiction'], [1572, 'contradiction'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'contradiction'], [1577, 'contradiction'], [1578, 'contradiction'], [1579, 'contradiction'], [1580, 'contradiction'], [1581, 'contradiction'], [1582, 'contradiction'], [1583, 'contradiction'], [1584, 'contradiction'], [1585, 'contradiction'], [1586, 'contradiction'], [1587, 'contradiction'], [1588, 'contradiction'], [1589, 'contradiction'], [1590, 'contradiction'], [1591, 'contradiction'], [1592, 'contradiction'], [1593, 'contradiction'], [1594, 'contradiction'], [1595, 'contradiction'], [1596, 'contradiction'], [1597, 'contradiction'], [1598, 'contradiction'], [1599, 'contradiction'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'contradiction'], [1604, 'contradiction'], [1605, 'contradiction'], [1606, 'contradiction'], [1607, 'contradiction'], [1608, 'contradiction'], [1609, 'contradiction'], [1610, 'contradiction'], [1611, 'contradiction'], [1612, 'contradiction'], [1613, 'contradiction'], [1614, 'contradiction'], [1615, 'contradiction'], [1616, 'contradiction'], [1617, 'contradiction'], [1618, 'contradiction'], [1619, 'contradiction'], [1620, 'contradiction'], [1621, 'contradiction'], [1622, 'contradiction'], [1623, 'contradiction'], [1624, 'contradiction'], [1625, 'contradiction'], [1626, 'contradiction'], [1627, 'contradiction'], [1628, 'contradiction'], [1629, 'contradiction'], [1630, 'contradiction'], [1631, 'contradiction'], [1632, 'contradiction'], [1633, 'contradiction'], [1634, 'contradiction'], [1635, 'contradiction'], [1636, 'contradiction'], [1637, 'contradiction'], [1638, 'contradiction'], [1639, 'contradiction'], [1640, 'contradiction'], [1641, 'contradiction'], [1642, 'contradiction'], [1643, 'contradiction'], [1644, 'contradiction'], [1645, 'contradiction'], [1646, 'contradiction'], [1647, 'contradiction'], [1648, 'contradiction'], [1649, 'contradiction'], [1650, 'contradiction'], [1651, 'contradiction'], [1652, 'contradiction'], [1653, 'contradiction'], [1654, 'contradiction'], [1655, 'contradiction'], [1656, 'contradiction'], [1657, 'contradiction'], [1658, 'contradiction'], [1659, 'contradiction'], [1660, 'contradiction'], [1661, 'contradiction'], [1662, 'contradiction'], [1663, 'contradiction'], [1664, 'contradiction'], [1665, 'contradiction']]\n",
      "      index          label\n",
      "0         0  contradiction\n",
      "1         1  contradiction\n",
      "2         2  contradiction\n",
      "3         3  contradiction\n",
      "4         4  contradiction\n",
      "...     ...            ...\n",
      "1661   1661  contradiction\n",
      "1662   1662  contradiction\n",
      "1663   1663  contradiction\n",
      "1664   1664  contradiction\n",
      "1665   1665  contradiction\n",
      "\n",
      "[1666 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_label = label_to_num(test['label'].values)\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(test['premise']),\n",
    "    list(test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDataset(tokenized_test, test_label)\n",
    "\n",
    "print(test_dataset.__len__())\n",
    "print(test_dataset.__getitem__(1665))\n",
    "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "print(pred_answer)\n",
    "\n",
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "answer = num_to_label(pred_answer)\n",
    "print(answer)\n",
    "\n",
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('./result/submission08.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3649fa4b9a6d1d3559da311d910871468d04a06554bfd4e3300b254822ca1a7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
