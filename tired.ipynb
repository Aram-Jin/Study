{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index                                            premise  \\\n",
      "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
      "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
      "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
      "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
      "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
      "\n",
      "                                hypothesis          label  \n",
      "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
      "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
      "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
      "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
      "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24998 entries, 0 to 24997\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       24998 non-null  int64 \n",
      " 1   premise     24998 non-null  object\n",
      " 2   hypothesis  24998 non-null  object\n",
      " 3   label       24998 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 781.3+ KB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1666 entries, 0 to 1665\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1666 non-null   int64 \n",
      " 1   premise     1666 non-null   object\n",
      " 2   hypothesis  1666 non-null   object\n",
      " 3   label       1666 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 52.2+ KB\n",
      "None\n",
      "Train Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Test Columns:  Index(['index', 'premise', 'hypothesis', 'label'], dtype='object')\n",
      "Train Label: \n",
      "entailment       8561\n",
      "contradiction    8489\n",
      "neutral          7948\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Test Label: \n",
      "answer    1666\n",
      "Name: label, dtype: int64\n",
      "Train Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      "Test Null: \n",
      "index         0\n",
      "premise       0\n",
      "hypothesis    0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Premise Length:  90\n",
      "Min Premise Length:  19\n",
      "Mean Premise Lenght:  45.406552524201935 \n",
      "\n",
      "Max Hypothesis Length:  103\n",
      "Min Hypothesis Length:  5\n",
      "Mean Hypothesis Lenght:  24.924433954716378\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dkfka\\AppData\\Local\\Temp/ipykernel_25952/1811701930.py:63: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "C:\\Users\\dkfka\\AppData\\Local\\Temp/ipykernel_25952/1811701930.py:64: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
      "C:\\Users\\dkfka\\AppData\\Local\\Temp/ipykernel_25952/1811701930.py:67: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
      "C:\\Users\\dkfka\\AppData\\Local\\Temp/ipykernel_25952/1811701930.py:68: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"klue/roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "tensor([    0, 30941,  2116, 11753,  5875, 12989,  2079, 14548,  2179,  3756,\n",
      "         6941,  1510,  2103,  2291,  6911,  2522,  1432,  2348,  2284,  2052,\n",
      "         7245,  3803,     2, 12989,  2145,  1510,  2103,  2291,  2073,  4273,\n",
      "         2470,  1536,  2052,  1415,  2062,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[CLS] 이정재가 삼성그룹 부회장 이재용의 전처인 대상그룹 임세령 상무와 열애중이라고 합니다 [SEP] 이재용과 임세령은 결혼한 적이 없다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "19998\n",
      "{'input_ids': tensor([    0, 22748,  2255, 19418, 12830,  2256,  2079, 16878,  6385,  7873,\n",
      "         2170,  3708,  4713,  2999,  7285,  7389,  2371,  2062,     2, 22748,\n",
      "         2255, 19418, 12830,  2256,  2259, 17291,  2318,  7873,  2371,  2062,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'label': tensor(0)}\n",
      "[CLS] 안드레스 이니에스타의 갑작스런 스페인 귀국에 일본 축구팬들이 당황했다 [SEP] 안드레스 이니에스타는 갑작스럽게 귀국했다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path =  \"D:/open/\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(path, 'train_data.csv'), encoding='utf-8')\n",
    "test = pd.read_csv(os.path.join(path, 'test_data.csv'), encoding='utf-8')\n",
    "\n",
    "print(train.head())\n",
    "print(train.info(), end='\\n\\n')\n",
    "print(test.info())\n",
    "print('Train Columns: ', train.columns)\n",
    "print('Test Columns: ', test.columns)\n",
    "print('Train Label: ', train['label'].value_counts(), sep='\\n', end='\\n\\n')\n",
    "print('Test Label: ', test['label'].value_counts(), sep='\\n')\n",
    "print('Train Null: ', train.isnull().sum(), sep='\\n', end='\\n\\n')\n",
    "print('Test Null: ', test.isnull().sum(), sep='\\n')\n",
    "\n",
    "feature = train['label']\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Label Count', fontsize=20)\n",
    "\n",
    "temp = feature.value_counts()\n",
    "plt.bar(temp.keys(), temp.values, width=0.5, color='b', alpha=0.5)\n",
    "plt.text(-0.05, temp.values[0]+20, s=temp.values[0])\n",
    "plt.text(0.95, temp.values[1]+20, s=temp.values[1])\n",
    "plt.text(1.95, temp.values[2]+20, s=temp.values[2])\n",
    "\n",
    "plt.xticks(temp.keys(), fontsize=12) # x축 값, 폰트 크기 설정\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "plt.show() # 그래프 나타내기\n",
    "\n",
    "max_len = np.max(train['premise'].str.len())\n",
    "min_len = np.min(train['premise'].str.len())\n",
    "mean_len = np.mean(train['premise'].str.len())\n",
    "\n",
    "print('Max Premise Length: ', max_len)\n",
    "print('Min Premise Length: ', min_len)\n",
    "print('Mean Premise Lenght: ', mean_len, '\\n')\n",
    "\n",
    "max_len = np.max(train['hypothesis'].str.len())\n",
    "min_len = np.min(train['hypothesis'].str.len())\n",
    "mean_len = np.mean(train['hypothesis'].str.len())\n",
    "\n",
    "print('Max Hypothesis Length: ', max_len)\n",
    "print('Min Hypothesis Length: ', min_len)\n",
    "print('Mean Hypothesis Lenght: ', mean_len)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "plt.title('Premise Length', fontsize=20)\n",
    "\n",
    "plt.hist(train['premise'].str.len(), alpha=0.5, color='orange')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # 레이아웃 설정\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['premise'] = test['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)\n",
    "\n",
    "train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
    "test['hypothesis'] = test['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
    "train.head(5)\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer\n",
    "\n",
    "def seed_everything(seed:int = 1004):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "MODEL_NAME = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 3\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "print(model)\n",
    "print(config)\n",
    "\n",
    "train_dataset, eval_dataset = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'])\n",
    "\n",
    "tokenized_train = tokenizer(\n",
    "    list(train_dataset['premise']),\n",
    "    list(train_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256, # Max_Length = 190\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokenized_eval = tokenizer(\n",
    "    list(eval_dataset['premise']),\n",
    "    list(eval_dataset['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "print(tokenized_train['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_train['input_ids'][0]))\n",
    "\n",
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pair_dataset, label):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['label'] = torch.tensor(self.label[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "def label_to_num(label):\n",
    "    label_dict = {\"entailment\": 0, \"contradiction\": 1, \"neutral\": 2, \"answer\": 3}\n",
    "    num_label = []\n",
    "\n",
    "    for v in label:\n",
    "        num_label.append(label_dict[v])\n",
    "    \n",
    "    return num_label\n",
    "\n",
    "\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "eval_label = label_to_num(eval_dataset['label'].values)\n",
    "\n",
    "train_dataset = BERTDataset(tokenized_train, train_label)\n",
    "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(train_dataset.__getitem__(19997))\n",
    "print(tokenizer.decode(train_dataset.__getitem__(19997)['input_ids']))\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validation을 위한 metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "  # calculate accuracy using sklearn's function\n",
    "    acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
    "\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "  }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ars = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 500,\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_ars,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19998\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4375\n",
      " 11%|█▏        | 500/4375 [07:36<46:57,  1.38it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1146, 'learning_rate': 4.428571428571428e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 11%|█▏        | 500/4375 [08:11<46:57,  1.38it/s]Saving model checkpoint to ./result\\checkpoint-500\n",
      "Configuration saved in ./result\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1100436449050903, 'eval_accuracy': 0.3424, 'eval_runtime': 34.3881, 'eval_samples_per_second': 145.399, 'eval_steps_per_second': 18.175, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-2000] due to args.save_total_limit\n",
      " 23%|██▎       | 1000/4375 [14:16<32:31,  1.73it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1081, 'learning_rate': 3.857142857142858e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 23%|██▎       | 1000/4375 [14:43<32:31,  1.73it/s]Saving model checkpoint to ./result\\checkpoint-1000\n",
      "Configuration saved in ./result\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1032185554504395, 'eval_accuracy': 0.3396, 'eval_runtime': 27.3658, 'eval_samples_per_second': 182.71, 'eval_steps_per_second': 22.839, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-1000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-2500] due to args.save_total_limit\n",
      " 34%|███▍      | 1500/4375 [20:11<27:58,  1.71it/s]   ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1065, 'learning_rate': 3.285714285714286e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 34%|███▍      | 1500/4375 [20:41<27:58,  1.71it/s]Saving model checkpoint to ./result\\checkpoint-1500\n",
      "Configuration saved in ./result\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.110990285873413, 'eval_accuracy': 0.3424, 'eval_runtime': 29.8111, 'eval_samples_per_second': 167.723, 'eval_steps_per_second': 20.965, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-1500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-3000] due to args.save_total_limit\n",
      " 46%|████▌     | 2000/4375 [26:04<29:35,  1.34it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1052, 'learning_rate': 2.714285714285714e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 46%|████▌     | 2000/4375 [26:39<29:35,  1.34it/s]Saving model checkpoint to ./result\\checkpoint-2000\n",
      "Configuration saved in ./result\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1009234189987183, 'eval_accuracy': 0.318, 'eval_runtime': 35.5681, 'eval_samples_per_second': 140.575, 'eval_steps_per_second': 17.572, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-2000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-3500] due to args.save_total_limit\n",
      " 57%|█████▋    | 2500/4375 [31:49<17:42,  1.76it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'learning_rate': 2.1428571428571428e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 57%|█████▋    | 2500/4375 [32:16<17:42,  1.76it/s]Saving model checkpoint to ./result\\checkpoint-2500\n",
      "Configuration saved in ./result\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0998115539550781, 'eval_accuracy': 0.3396, 'eval_runtime': 26.8936, 'eval_samples_per_second': 185.918, 'eval_steps_per_second': 23.24, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-2500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-4000] due to args.save_total_limit\n",
      " 69%|██████▊   | 3000/4375 [37:21<13:20,  1.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1028, 'learning_rate': 1.5714285714285715e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 69%|██████▊   | 3000/4375 [37:49<13:20,  1.72it/s]Saving model checkpoint to ./result\\checkpoint-3000\n",
      "Configuration saved in ./result\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1026862859725952, 'eval_accuracy': 0.318, 'eval_runtime': 27.7829, 'eval_samples_per_second': 179.967, 'eval_steps_per_second': 22.496, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-3000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-500] due to args.save_total_limit\n",
      " 80%|████████  | 3500/4375 [43:07<08:29,  1.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1016, 'learning_rate': 1e-05, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 3500/4375 [43:35<08:29,  1.72it/s]Saving model checkpoint to ./result\\checkpoint-3500\n",
      "Configuration saved in ./result\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0995724201202393, 'eval_accuracy': 0.3424, 'eval_runtime': 28.139, 'eval_samples_per_second': 177.689, 'eval_steps_per_second': 22.211, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-3500\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-1000] due to args.save_total_limit\n",
      " 91%|█████████▏| 4000/4375 [48:59<04:37,  1.35it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0996, 'learning_rate': 4.285714285714286e-06, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████▏| 4000/4375 [49:34<04:37,  1.35it/s]Saving model checkpoint to ./result\\checkpoint-4000\n",
      "Configuration saved in ./result\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0982393026351929, 'eval_accuracy': 0.3396, 'eval_runtime': 35.0974, 'eval_samples_per_second': 142.461, 'eval_steps_per_second': 17.808, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in ./result\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in ./result\\checkpoint-4000\\special_tokens_map.json\n",
      "Deleting older checkpoint [result\\checkpoint-1500] due to args.save_total_limit\n",
      "100%|██████████| 4375/4375 [54:17<00:00,  1.41it/s]  \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./result\\checkpoint-4000 (score: 1.0982393026351929).\n",
      "100%|██████████| 4375/4375 [54:19<00:00,  1.34it/s]\n",
      "Configuration saved in ./result/checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3259.1322, 'train_samples_per_second': 42.952, 'train_steps_per_second': 1.342, 'train_loss': 1.104704750279018, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./result/checkpoint-4000\\pytorch_model.bin\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at C:\\Users\\dkfka/.cache\\huggingface\\transformers\\4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at C:\\Users\\dkfka/.cache\\huggingface\\transformers\\360b579947002f14f22331a026821b56f70679f1be1e95fe5dc5a80edc4a59e0.44c30ade4958fcfd446e66025e10a5b380cdd0bbe9b3fb7a794f357e7f0f34c2\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at C:\\Users\\dkfka/.cache\\huggingface\\transformers\\1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at C:\\Users\\dkfka/.cache\\huggingface\\transformers\\8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.5b0ba083b234382bb4c99ee0c9f4fca4cadaa053dd17c32dabfe0de2f629af1f\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('./result/checkpoint-4000')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "Tokenizer_NAME = \"klue/roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./result/best_model\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./result/best_model\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./result/best_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./result/best_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = './result/best_model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.resize_token_embeddings(tokenizer.vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1666\n",
      "{'input_ids': tensor([    0,   720,  3994,  2052, 10428,  2775,   647,  3657,  2119,  1085,\n",
      "            3,     2,   720,  3994,  2052,   911,  2075,  3669,  2119,  3926,\n",
      "         2088,  1513,  2359, 13964,     2,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3)}\n",
      "[CLS] 18일 귀국이라 발인도 지켜드리지 못해 더욱 죄송할 따름입니다 [SEP] 18일 배를 타고 여행을 떠났습니다 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:10<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[0, 'contradiction'], [1, 'contradiction'], [2, 'contradiction'], [3, 'contradiction'], [4, 'contradiction'], [5, 'contradiction'], [6, 'contradiction'], [7, 'contradiction'], [8, 'contradiction'], [9, 'contradiction'], [10, 'contradiction'], [11, 'contradiction'], [12, 'contradiction'], [13, 'contradiction'], [14, 'contradiction'], [15, 'contradiction'], [16, 'contradiction'], [17, 'contradiction'], [18, 'contradiction'], [19, 'contradiction'], [20, 'contradiction'], [21, 'contradiction'], [22, 'contradiction'], [23, 'contradiction'], [24, 'contradiction'], [25, 'contradiction'], [26, 'contradiction'], [27, 'contradiction'], [28, 'contradiction'], [29, 'contradiction'], [30, 'contradiction'], [31, 'contradiction'], [32, 'contradiction'], [33, 'contradiction'], [34, 'contradiction'], [35, 'contradiction'], [36, 'contradiction'], [37, 'contradiction'], [38, 'contradiction'], [39, 'contradiction'], [40, 'contradiction'], [41, 'contradiction'], [42, 'contradiction'], [43, 'contradiction'], [44, 'contradiction'], [45, 'contradiction'], [46, 'contradiction'], [47, 'contradiction'], [48, 'contradiction'], [49, 'contradiction'], [50, 'contradiction'], [51, 'contradiction'], [52, 'contradiction'], [53, 'contradiction'], [54, 'contradiction'], [55, 'contradiction'], [56, 'contradiction'], [57, 'contradiction'], [58, 'contradiction'], [59, 'contradiction'], [60, 'contradiction'], [61, 'contradiction'], [62, 'contradiction'], [63, 'contradiction'], [64, 'contradiction'], [65, 'contradiction'], [66, 'contradiction'], [67, 'contradiction'], [68, 'contradiction'], [69, 'contradiction'], [70, 'contradiction'], [71, 'contradiction'], [72, 'contradiction'], [73, 'contradiction'], [74, 'contradiction'], [75, 'contradiction'], [76, 'contradiction'], [77, 'contradiction'], [78, 'contradiction'], [79, 'contradiction'], [80, 'contradiction'], [81, 'contradiction'], [82, 'contradiction'], [83, 'contradiction'], [84, 'contradiction'], [85, 'contradiction'], [86, 'contradiction'], [87, 'contradiction'], [88, 'contradiction'], [89, 'contradiction'], [90, 'contradiction'], [91, 'contradiction'], [92, 'contradiction'], [93, 'contradiction'], [94, 'contradiction'], [95, 'contradiction'], [96, 'contradiction'], [97, 'contradiction'], [98, 'contradiction'], [99, 'contradiction'], [100, 'contradiction'], [101, 'contradiction'], [102, 'contradiction'], [103, 'contradiction'], [104, 'contradiction'], [105, 'contradiction'], [106, 'contradiction'], [107, 'contradiction'], [108, 'contradiction'], [109, 'contradiction'], [110, 'contradiction'], [111, 'contradiction'], [112, 'contradiction'], [113, 'contradiction'], [114, 'contradiction'], [115, 'contradiction'], [116, 'contradiction'], [117, 'contradiction'], [118, 'contradiction'], [119, 'contradiction'], [120, 'contradiction'], [121, 'contradiction'], [122, 'contradiction'], [123, 'contradiction'], [124, 'contradiction'], [125, 'contradiction'], [126, 'contradiction'], [127, 'contradiction'], [128, 'contradiction'], [129, 'contradiction'], [130, 'contradiction'], [131, 'contradiction'], [132, 'contradiction'], [133, 'contradiction'], [134, 'contradiction'], [135, 'contradiction'], [136, 'contradiction'], [137, 'contradiction'], [138, 'contradiction'], [139, 'contradiction'], [140, 'contradiction'], [141, 'contradiction'], [142, 'contradiction'], [143, 'contradiction'], [144, 'contradiction'], [145, 'contradiction'], [146, 'contradiction'], [147, 'contradiction'], [148, 'contradiction'], [149, 'contradiction'], [150, 'contradiction'], [151, 'contradiction'], [152, 'contradiction'], [153, 'contradiction'], [154, 'contradiction'], [155, 'contradiction'], [156, 'contradiction'], [157, 'contradiction'], [158, 'contradiction'], [159, 'contradiction'], [160, 'contradiction'], [161, 'contradiction'], [162, 'contradiction'], [163, 'contradiction'], [164, 'contradiction'], [165, 'contradiction'], [166, 'contradiction'], [167, 'contradiction'], [168, 'contradiction'], [169, 'contradiction'], [170, 'contradiction'], [171, 'contradiction'], [172, 'contradiction'], [173, 'contradiction'], [174, 'contradiction'], [175, 'contradiction'], [176, 'contradiction'], [177, 'contradiction'], [178, 'contradiction'], [179, 'contradiction'], [180, 'contradiction'], [181, 'contradiction'], [182, 'contradiction'], [183, 'contradiction'], [184, 'contradiction'], [185, 'contradiction'], [186, 'contradiction'], [187, 'contradiction'], [188, 'contradiction'], [189, 'contradiction'], [190, 'contradiction'], [191, 'contradiction'], [192, 'contradiction'], [193, 'contradiction'], [194, 'contradiction'], [195, 'contradiction'], [196, 'contradiction'], [197, 'contradiction'], [198, 'contradiction'], [199, 'contradiction'], [200, 'contradiction'], [201, 'contradiction'], [202, 'contradiction'], [203, 'contradiction'], [204, 'contradiction'], [205, 'contradiction'], [206, 'contradiction'], [207, 'contradiction'], [208, 'contradiction'], [209, 'contradiction'], [210, 'contradiction'], [211, 'contradiction'], [212, 'contradiction'], [213, 'contradiction'], [214, 'contradiction'], [215, 'contradiction'], [216, 'contradiction'], [217, 'contradiction'], [218, 'contradiction'], [219, 'contradiction'], [220, 'contradiction'], [221, 'contradiction'], [222, 'contradiction'], [223, 'contradiction'], [224, 'contradiction'], [225, 'contradiction'], [226, 'contradiction'], [227, 'contradiction'], [228, 'contradiction'], [229, 'contradiction'], [230, 'contradiction'], [231, 'contradiction'], [232, 'contradiction'], [233, 'contradiction'], [234, 'contradiction'], [235, 'contradiction'], [236, 'contradiction'], [237, 'contradiction'], [238, 'contradiction'], [239, 'contradiction'], [240, 'contradiction'], [241, 'contradiction'], [242, 'contradiction'], [243, 'contradiction'], [244, 'contradiction'], [245, 'contradiction'], [246, 'contradiction'], [247, 'contradiction'], [248, 'contradiction'], [249, 'contradiction'], [250, 'contradiction'], [251, 'contradiction'], [252, 'contradiction'], [253, 'contradiction'], [254, 'contradiction'], [255, 'contradiction'], [256, 'contradiction'], [257, 'contradiction'], [258, 'contradiction'], [259, 'contradiction'], [260, 'contradiction'], [261, 'contradiction'], [262, 'contradiction'], [263, 'contradiction'], [264, 'contradiction'], [265, 'contradiction'], [266, 'contradiction'], [267, 'contradiction'], [268, 'contradiction'], [269, 'contradiction'], [270, 'contradiction'], [271, 'contradiction'], [272, 'contradiction'], [273, 'contradiction'], [274, 'contradiction'], [275, 'contradiction'], [276, 'contradiction'], [277, 'contradiction'], [278, 'contradiction'], [279, 'contradiction'], [280, 'contradiction'], [281, 'contradiction'], [282, 'contradiction'], [283, 'contradiction'], [284, 'contradiction'], [285, 'contradiction'], [286, 'contradiction'], [287, 'contradiction'], [288, 'contradiction'], [289, 'contradiction'], [290, 'contradiction'], [291, 'contradiction'], [292, 'contradiction'], [293, 'contradiction'], [294, 'contradiction'], [295, 'contradiction'], [296, 'contradiction'], [297, 'contradiction'], [298, 'contradiction'], [299, 'contradiction'], [300, 'contradiction'], [301, 'contradiction'], [302, 'contradiction'], [303, 'contradiction'], [304, 'contradiction'], [305, 'contradiction'], [306, 'contradiction'], [307, 'contradiction'], [308, 'contradiction'], [309, 'contradiction'], [310, 'contradiction'], [311, 'contradiction'], [312, 'contradiction'], [313, 'contradiction'], [314, 'contradiction'], [315, 'contradiction'], [316, 'contradiction'], [317, 'contradiction'], [318, 'contradiction'], [319, 'contradiction'], [320, 'contradiction'], [321, 'contradiction'], [322, 'contradiction'], [323, 'contradiction'], [324, 'contradiction'], [325, 'contradiction'], [326, 'contradiction'], [327, 'contradiction'], [328, 'contradiction'], [329, 'contradiction'], [330, 'contradiction'], [331, 'contradiction'], [332, 'contradiction'], [333, 'contradiction'], [334, 'contradiction'], [335, 'contradiction'], [336, 'contradiction'], [337, 'contradiction'], [338, 'contradiction'], [339, 'contradiction'], [340, 'contradiction'], [341, 'contradiction'], [342, 'contradiction'], [343, 'contradiction'], [344, 'contradiction'], [345, 'contradiction'], [346, 'contradiction'], [347, 'contradiction'], [348, 'contradiction'], [349, 'contradiction'], [350, 'contradiction'], [351, 'contradiction'], [352, 'contradiction'], [353, 'contradiction'], [354, 'contradiction'], [355, 'contradiction'], [356, 'contradiction'], [357, 'contradiction'], [358, 'contradiction'], [359, 'contradiction'], [360, 'contradiction'], [361, 'contradiction'], [362, 'contradiction'], [363, 'contradiction'], [364, 'contradiction'], [365, 'contradiction'], [366, 'contradiction'], [367, 'contradiction'], [368, 'contradiction'], [369, 'contradiction'], [370, 'contradiction'], [371, 'contradiction'], [372, 'contradiction'], [373, 'contradiction'], [374, 'contradiction'], [375, 'contradiction'], [376, 'contradiction'], [377, 'contradiction'], [378, 'contradiction'], [379, 'contradiction'], [380, 'contradiction'], [381, 'contradiction'], [382, 'contradiction'], [383, 'contradiction'], [384, 'contradiction'], [385, 'contradiction'], [386, 'contradiction'], [387, 'contradiction'], [388, 'contradiction'], [389, 'contradiction'], [390, 'contradiction'], [391, 'contradiction'], [392, 'contradiction'], [393, 'contradiction'], [394, 'contradiction'], [395, 'contradiction'], [396, 'contradiction'], [397, 'contradiction'], [398, 'contradiction'], [399, 'contradiction'], [400, 'contradiction'], [401, 'contradiction'], [402, 'contradiction'], [403, 'contradiction'], [404, 'contradiction'], [405, 'contradiction'], [406, 'contradiction'], [407, 'contradiction'], [408, 'contradiction'], [409, 'contradiction'], [410, 'contradiction'], [411, 'contradiction'], [412, 'contradiction'], [413, 'contradiction'], [414, 'contradiction'], [415, 'contradiction'], [416, 'contradiction'], [417, 'contradiction'], [418, 'contradiction'], [419, 'contradiction'], [420, 'contradiction'], [421, 'contradiction'], [422, 'contradiction'], [423, 'contradiction'], [424, 'contradiction'], [425, 'contradiction'], [426, 'contradiction'], [427, 'contradiction'], [428, 'contradiction'], [429, 'contradiction'], [430, 'contradiction'], [431, 'contradiction'], [432, 'contradiction'], [433, 'contradiction'], [434, 'contradiction'], [435, 'contradiction'], [436, 'contradiction'], [437, 'contradiction'], [438, 'contradiction'], [439, 'contradiction'], [440, 'contradiction'], [441, 'contradiction'], [442, 'contradiction'], [443, 'contradiction'], [444, 'contradiction'], [445, 'contradiction'], [446, 'contradiction'], [447, 'contradiction'], [448, 'contradiction'], [449, 'contradiction'], [450, 'contradiction'], [451, 'contradiction'], [452, 'contradiction'], [453, 'contradiction'], [454, 'contradiction'], [455, 'contradiction'], [456, 'contradiction'], [457, 'contradiction'], [458, 'contradiction'], [459, 'contradiction'], [460, 'contradiction'], [461, 'contradiction'], [462, 'contradiction'], [463, 'contradiction'], [464, 'contradiction'], [465, 'contradiction'], [466, 'contradiction'], [467, 'contradiction'], [468, 'contradiction'], [469, 'contradiction'], [470, 'contradiction'], [471, 'contradiction'], [472, 'contradiction'], [473, 'contradiction'], [474, 'contradiction'], [475, 'contradiction'], [476, 'contradiction'], [477, 'contradiction'], [478, 'contradiction'], [479, 'contradiction'], [480, 'contradiction'], [481, 'contradiction'], [482, 'contradiction'], [483, 'contradiction'], [484, 'contradiction'], [485, 'contradiction'], [486, 'contradiction'], [487, 'contradiction'], [488, 'contradiction'], [489, 'contradiction'], [490, 'contradiction'], [491, 'contradiction'], [492, 'contradiction'], [493, 'contradiction'], [494, 'contradiction'], [495, 'contradiction'], [496, 'contradiction'], [497, 'contradiction'], [498, 'contradiction'], [499, 'contradiction'], [500, 'contradiction'], [501, 'contradiction'], [502, 'contradiction'], [503, 'contradiction'], [504, 'contradiction'], [505, 'contradiction'], [506, 'contradiction'], [507, 'contradiction'], [508, 'contradiction'], [509, 'contradiction'], [510, 'contradiction'], [511, 'contradiction'], [512, 'contradiction'], [513, 'contradiction'], [514, 'contradiction'], [515, 'contradiction'], [516, 'contradiction'], [517, 'contradiction'], [518, 'contradiction'], [519, 'contradiction'], [520, 'contradiction'], [521, 'contradiction'], [522, 'contradiction'], [523, 'contradiction'], [524, 'contradiction'], [525, 'contradiction'], [526, 'contradiction'], [527, 'contradiction'], [528, 'contradiction'], [529, 'contradiction'], [530, 'contradiction'], [531, 'contradiction'], [532, 'contradiction'], [533, 'contradiction'], [534, 'contradiction'], [535, 'contradiction'], [536, 'contradiction'], [537, 'contradiction'], [538, 'contradiction'], [539, 'contradiction'], [540, 'contradiction'], [541, 'contradiction'], [542, 'contradiction'], [543, 'contradiction'], [544, 'contradiction'], [545, 'contradiction'], [546, 'contradiction'], [547, 'contradiction'], [548, 'contradiction'], [549, 'contradiction'], [550, 'contradiction'], [551, 'contradiction'], [552, 'contradiction'], [553, 'contradiction'], [554, 'contradiction'], [555, 'contradiction'], [556, 'contradiction'], [557, 'contradiction'], [558, 'contradiction'], [559, 'contradiction'], [560, 'contradiction'], [561, 'contradiction'], [562, 'contradiction'], [563, 'contradiction'], [564, 'contradiction'], [565, 'contradiction'], [566, 'contradiction'], [567, 'contradiction'], [568, 'contradiction'], [569, 'contradiction'], [570, 'contradiction'], [571, 'contradiction'], [572, 'contradiction'], [573, 'contradiction'], [574, 'contradiction'], [575, 'contradiction'], [576, 'contradiction'], [577, 'contradiction'], [578, 'contradiction'], [579, 'contradiction'], [580, 'contradiction'], [581, 'contradiction'], [582, 'contradiction'], [583, 'contradiction'], [584, 'contradiction'], [585, 'contradiction'], [586, 'contradiction'], [587, 'contradiction'], [588, 'contradiction'], [589, 'contradiction'], [590, 'contradiction'], [591, 'contradiction'], [592, 'contradiction'], [593, 'contradiction'], [594, 'contradiction'], [595, 'contradiction'], [596, 'contradiction'], [597, 'contradiction'], [598, 'contradiction'], [599, 'contradiction'], [600, 'contradiction'], [601, 'contradiction'], [602, 'contradiction'], [603, 'contradiction'], [604, 'contradiction'], [605, 'contradiction'], [606, 'contradiction'], [607, 'contradiction'], [608, 'contradiction'], [609, 'contradiction'], [610, 'contradiction'], [611, 'contradiction'], [612, 'contradiction'], [613, 'contradiction'], [614, 'contradiction'], [615, 'contradiction'], [616, 'contradiction'], [617, 'contradiction'], [618, 'contradiction'], [619, 'contradiction'], [620, 'contradiction'], [621, 'contradiction'], [622, 'contradiction'], [623, 'contradiction'], [624, 'contradiction'], [625, 'contradiction'], [626, 'contradiction'], [627, 'contradiction'], [628, 'contradiction'], [629, 'contradiction'], [630, 'contradiction'], [631, 'contradiction'], [632, 'contradiction'], [633, 'contradiction'], [634, 'contradiction'], [635, 'contradiction'], [636, 'contradiction'], [637, 'contradiction'], [638, 'contradiction'], [639, 'contradiction'], [640, 'contradiction'], [641, 'contradiction'], [642, 'contradiction'], [643, 'contradiction'], [644, 'contradiction'], [645, 'contradiction'], [646, 'contradiction'], [647, 'contradiction'], [648, 'contradiction'], [649, 'contradiction'], [650, 'contradiction'], [651, 'contradiction'], [652, 'contradiction'], [653, 'contradiction'], [654, 'contradiction'], [655, 'contradiction'], [656, 'contradiction'], [657, 'contradiction'], [658, 'contradiction'], [659, 'contradiction'], [660, 'contradiction'], [661, 'contradiction'], [662, 'contradiction'], [663, 'contradiction'], [664, 'contradiction'], [665, 'contradiction'], [666, 'contradiction'], [667, 'contradiction'], [668, 'contradiction'], [669, 'contradiction'], [670, 'contradiction'], [671, 'contradiction'], [672, 'contradiction'], [673, 'contradiction'], [674, 'contradiction'], [675, 'contradiction'], [676, 'contradiction'], [677, 'contradiction'], [678, 'contradiction'], [679, 'contradiction'], [680, 'contradiction'], [681, 'contradiction'], [682, 'contradiction'], [683, 'contradiction'], [684, 'contradiction'], [685, 'contradiction'], [686, 'contradiction'], [687, 'contradiction'], [688, 'contradiction'], [689, 'contradiction'], [690, 'contradiction'], [691, 'contradiction'], [692, 'contradiction'], [693, 'contradiction'], [694, 'contradiction'], [695, 'contradiction'], [696, 'contradiction'], [697, 'contradiction'], [698, 'contradiction'], [699, 'contradiction'], [700, 'contradiction'], [701, 'contradiction'], [702, 'contradiction'], [703, 'contradiction'], [704, 'contradiction'], [705, 'contradiction'], [706, 'contradiction'], [707, 'contradiction'], [708, 'contradiction'], [709, 'contradiction'], [710, 'contradiction'], [711, 'contradiction'], [712, 'contradiction'], [713, 'contradiction'], [714, 'contradiction'], [715, 'contradiction'], [716, 'contradiction'], [717, 'contradiction'], [718, 'contradiction'], [719, 'contradiction'], [720, 'contradiction'], [721, 'contradiction'], [722, 'contradiction'], [723, 'contradiction'], [724, 'contradiction'], [725, 'contradiction'], [726, 'contradiction'], [727, 'contradiction'], [728, 'contradiction'], [729, 'contradiction'], [730, 'contradiction'], [731, 'contradiction'], [732, 'contradiction'], [733, 'contradiction'], [734, 'contradiction'], [735, 'contradiction'], [736, 'contradiction'], [737, 'contradiction'], [738, 'contradiction'], [739, 'contradiction'], [740, 'contradiction'], [741, 'contradiction'], [742, 'contradiction'], [743, 'contradiction'], [744, 'contradiction'], [745, 'contradiction'], [746, 'contradiction'], [747, 'contradiction'], [748, 'contradiction'], [749, 'contradiction'], [750, 'contradiction'], [751, 'contradiction'], [752, 'contradiction'], [753, 'contradiction'], [754, 'contradiction'], [755, 'contradiction'], [756, 'contradiction'], [757, 'contradiction'], [758, 'contradiction'], [759, 'contradiction'], [760, 'contradiction'], [761, 'contradiction'], [762, 'contradiction'], [763, 'contradiction'], [764, 'contradiction'], [765, 'contradiction'], [766, 'contradiction'], [767, 'contradiction'], [768, 'contradiction'], [769, 'contradiction'], [770, 'contradiction'], [771, 'contradiction'], [772, 'contradiction'], [773, 'contradiction'], [774, 'contradiction'], [775, 'contradiction'], [776, 'contradiction'], [777, 'contradiction'], [778, 'contradiction'], [779, 'contradiction'], [780, 'contradiction'], [781, 'contradiction'], [782, 'contradiction'], [783, 'contradiction'], [784, 'contradiction'], [785, 'contradiction'], [786, 'contradiction'], [787, 'contradiction'], [788, 'contradiction'], [789, 'contradiction'], [790, 'contradiction'], [791, 'contradiction'], [792, 'contradiction'], [793, 'contradiction'], [794, 'contradiction'], [795, 'contradiction'], [796, 'contradiction'], [797, 'contradiction'], [798, 'contradiction'], [799, 'contradiction'], [800, 'contradiction'], [801, 'contradiction'], [802, 'contradiction'], [803, 'contradiction'], [804, 'contradiction'], [805, 'contradiction'], [806, 'contradiction'], [807, 'contradiction'], [808, 'contradiction'], [809, 'contradiction'], [810, 'contradiction'], [811, 'contradiction'], [812, 'contradiction'], [813, 'contradiction'], [814, 'contradiction'], [815, 'contradiction'], [816, 'contradiction'], [817, 'contradiction'], [818, 'contradiction'], [819, 'contradiction'], [820, 'contradiction'], [821, 'contradiction'], [822, 'contradiction'], [823, 'contradiction'], [824, 'contradiction'], [825, 'contradiction'], [826, 'contradiction'], [827, 'contradiction'], [828, 'contradiction'], [829, 'contradiction'], [830, 'contradiction'], [831, 'contradiction'], [832, 'contradiction'], [833, 'contradiction'], [834, 'contradiction'], [835, 'contradiction'], [836, 'contradiction'], [837, 'contradiction'], [838, 'contradiction'], [839, 'contradiction'], [840, 'contradiction'], [841, 'contradiction'], [842, 'contradiction'], [843, 'contradiction'], [844, 'contradiction'], [845, 'contradiction'], [846, 'contradiction'], [847, 'contradiction'], [848, 'contradiction'], [849, 'contradiction'], [850, 'contradiction'], [851, 'contradiction'], [852, 'contradiction'], [853, 'contradiction'], [854, 'contradiction'], [855, 'contradiction'], [856, 'contradiction'], [857, 'contradiction'], [858, 'contradiction'], [859, 'contradiction'], [860, 'contradiction'], [861, 'contradiction'], [862, 'contradiction'], [863, 'contradiction'], [864, 'contradiction'], [865, 'contradiction'], [866, 'contradiction'], [867, 'contradiction'], [868, 'contradiction'], [869, 'contradiction'], [870, 'contradiction'], [871, 'contradiction'], [872, 'contradiction'], [873, 'contradiction'], [874, 'contradiction'], [875, 'contradiction'], [876, 'contradiction'], [877, 'contradiction'], [878, 'contradiction'], [879, 'contradiction'], [880, 'contradiction'], [881, 'contradiction'], [882, 'contradiction'], [883, 'contradiction'], [884, 'contradiction'], [885, 'contradiction'], [886, 'contradiction'], [887, 'contradiction'], [888, 'contradiction'], [889, 'contradiction'], [890, 'contradiction'], [891, 'contradiction'], [892, 'contradiction'], [893, 'contradiction'], [894, 'contradiction'], [895, 'contradiction'], [896, 'contradiction'], [897, 'contradiction'], [898, 'contradiction'], [899, 'contradiction'], [900, 'contradiction'], [901, 'contradiction'], [902, 'contradiction'], [903, 'contradiction'], [904, 'contradiction'], [905, 'contradiction'], [906, 'contradiction'], [907, 'contradiction'], [908, 'contradiction'], [909, 'contradiction'], [910, 'contradiction'], [911, 'contradiction'], [912, 'contradiction'], [913, 'contradiction'], [914, 'contradiction'], [915, 'contradiction'], [916, 'contradiction'], [917, 'contradiction'], [918, 'contradiction'], [919, 'contradiction'], [920, 'contradiction'], [921, 'contradiction'], [922, 'contradiction'], [923, 'contradiction'], [924, 'contradiction'], [925, 'contradiction'], [926, 'contradiction'], [927, 'contradiction'], [928, 'contradiction'], [929, 'contradiction'], [930, 'contradiction'], [931, 'contradiction'], [932, 'contradiction'], [933, 'contradiction'], [934, 'contradiction'], [935, 'contradiction'], [936, 'contradiction'], [937, 'contradiction'], [938, 'contradiction'], [939, 'contradiction'], [940, 'contradiction'], [941, 'contradiction'], [942, 'contradiction'], [943, 'contradiction'], [944, 'contradiction'], [945, 'contradiction'], [946, 'contradiction'], [947, 'contradiction'], [948, 'contradiction'], [949, 'contradiction'], [950, 'contradiction'], [951, 'contradiction'], [952, 'contradiction'], [953, 'contradiction'], [954, 'contradiction'], [955, 'contradiction'], [956, 'contradiction'], [957, 'contradiction'], [958, 'contradiction'], [959, 'contradiction'], [960, 'contradiction'], [961, 'contradiction'], [962, 'contradiction'], [963, 'contradiction'], [964, 'contradiction'], [965, 'contradiction'], [966, 'contradiction'], [967, 'contradiction'], [968, 'contradiction'], [969, 'contradiction'], [970, 'contradiction'], [971, 'contradiction'], [972, 'contradiction'], [973, 'contradiction'], [974, 'contradiction'], [975, 'contradiction'], [976, 'contradiction'], [977, 'contradiction'], [978, 'contradiction'], [979, 'contradiction'], [980, 'contradiction'], [981, 'contradiction'], [982, 'contradiction'], [983, 'contradiction'], [984, 'contradiction'], [985, 'contradiction'], [986, 'contradiction'], [987, 'contradiction'], [988, 'contradiction'], [989, 'contradiction'], [990, 'contradiction'], [991, 'contradiction'], [992, 'contradiction'], [993, 'contradiction'], [994, 'contradiction'], [995, 'contradiction'], [996, 'contradiction'], [997, 'contradiction'], [998, 'contradiction'], [999, 'contradiction'], [1000, 'contradiction'], [1001, 'contradiction'], [1002, 'contradiction'], [1003, 'contradiction'], [1004, 'contradiction'], [1005, 'contradiction'], [1006, 'contradiction'], [1007, 'contradiction'], [1008, 'contradiction'], [1009, 'contradiction'], [1010, 'contradiction'], [1011, 'contradiction'], [1012, 'contradiction'], [1013, 'contradiction'], [1014, 'contradiction'], [1015, 'contradiction'], [1016, 'contradiction'], [1017, 'contradiction'], [1018, 'contradiction'], [1019, 'contradiction'], [1020, 'contradiction'], [1021, 'contradiction'], [1022, 'contradiction'], [1023, 'contradiction'], [1024, 'contradiction'], [1025, 'contradiction'], [1026, 'contradiction'], [1027, 'contradiction'], [1028, 'contradiction'], [1029, 'contradiction'], [1030, 'contradiction'], [1031, 'contradiction'], [1032, 'contradiction'], [1033, 'contradiction'], [1034, 'contradiction'], [1035, 'contradiction'], [1036, 'contradiction'], [1037, 'contradiction'], [1038, 'contradiction'], [1039, 'contradiction'], [1040, 'contradiction'], [1041, 'contradiction'], [1042, 'contradiction'], [1043, 'contradiction'], [1044, 'contradiction'], [1045, 'contradiction'], [1046, 'contradiction'], [1047, 'contradiction'], [1048, 'contradiction'], [1049, 'contradiction'], [1050, 'contradiction'], [1051, 'contradiction'], [1052, 'contradiction'], [1053, 'contradiction'], [1054, 'contradiction'], [1055, 'contradiction'], [1056, 'contradiction'], [1057, 'contradiction'], [1058, 'contradiction'], [1059, 'contradiction'], [1060, 'contradiction'], [1061, 'contradiction'], [1062, 'contradiction'], [1063, 'contradiction'], [1064, 'contradiction'], [1065, 'contradiction'], [1066, 'contradiction'], [1067, 'contradiction'], [1068, 'contradiction'], [1069, 'contradiction'], [1070, 'contradiction'], [1071, 'contradiction'], [1072, 'contradiction'], [1073, 'contradiction'], [1074, 'contradiction'], [1075, 'contradiction'], [1076, 'contradiction'], [1077, 'contradiction'], [1078, 'contradiction'], [1079, 'contradiction'], [1080, 'contradiction'], [1081, 'contradiction'], [1082, 'contradiction'], [1083, 'contradiction'], [1084, 'contradiction'], [1085, 'contradiction'], [1086, 'contradiction'], [1087, 'contradiction'], [1088, 'contradiction'], [1089, 'contradiction'], [1090, 'contradiction'], [1091, 'contradiction'], [1092, 'contradiction'], [1093, 'contradiction'], [1094, 'contradiction'], [1095, 'contradiction'], [1096, 'contradiction'], [1097, 'contradiction'], [1098, 'contradiction'], [1099, 'contradiction'], [1100, 'contradiction'], [1101, 'contradiction'], [1102, 'contradiction'], [1103, 'contradiction'], [1104, 'contradiction'], [1105, 'contradiction'], [1106, 'contradiction'], [1107, 'contradiction'], [1108, 'contradiction'], [1109, 'contradiction'], [1110, 'contradiction'], [1111, 'contradiction'], [1112, 'contradiction'], [1113, 'contradiction'], [1114, 'contradiction'], [1115, 'contradiction'], [1116, 'contradiction'], [1117, 'contradiction'], [1118, 'contradiction'], [1119, 'contradiction'], [1120, 'contradiction'], [1121, 'contradiction'], [1122, 'contradiction'], [1123, 'contradiction'], [1124, 'contradiction'], [1125, 'contradiction'], [1126, 'contradiction'], [1127, 'contradiction'], [1128, 'contradiction'], [1129, 'contradiction'], [1130, 'contradiction'], [1131, 'contradiction'], [1132, 'contradiction'], [1133, 'contradiction'], [1134, 'contradiction'], [1135, 'contradiction'], [1136, 'contradiction'], [1137, 'contradiction'], [1138, 'contradiction'], [1139, 'contradiction'], [1140, 'contradiction'], [1141, 'contradiction'], [1142, 'contradiction'], [1143, 'contradiction'], [1144, 'contradiction'], [1145, 'contradiction'], [1146, 'contradiction'], [1147, 'contradiction'], [1148, 'contradiction'], [1149, 'contradiction'], [1150, 'contradiction'], [1151, 'contradiction'], [1152, 'contradiction'], [1153, 'contradiction'], [1154, 'contradiction'], [1155, 'contradiction'], [1156, 'contradiction'], [1157, 'contradiction'], [1158, 'contradiction'], [1159, 'contradiction'], [1160, 'contradiction'], [1161, 'contradiction'], [1162, 'contradiction'], [1163, 'contradiction'], [1164, 'contradiction'], [1165, 'contradiction'], [1166, 'contradiction'], [1167, 'contradiction'], [1168, 'contradiction'], [1169, 'contradiction'], [1170, 'contradiction'], [1171, 'contradiction'], [1172, 'contradiction'], [1173, 'contradiction'], [1174, 'contradiction'], [1175, 'contradiction'], [1176, 'contradiction'], [1177, 'contradiction'], [1178, 'contradiction'], [1179, 'contradiction'], [1180, 'contradiction'], [1181, 'contradiction'], [1182, 'contradiction'], [1183, 'contradiction'], [1184, 'contradiction'], [1185, 'contradiction'], [1186, 'contradiction'], [1187, 'contradiction'], [1188, 'contradiction'], [1189, 'contradiction'], [1190, 'contradiction'], [1191, 'contradiction'], [1192, 'contradiction'], [1193, 'contradiction'], [1194, 'contradiction'], [1195, 'contradiction'], [1196, 'contradiction'], [1197, 'contradiction'], [1198, 'contradiction'], [1199, 'contradiction'], [1200, 'contradiction'], [1201, 'contradiction'], [1202, 'contradiction'], [1203, 'contradiction'], [1204, 'contradiction'], [1205, 'contradiction'], [1206, 'contradiction'], [1207, 'contradiction'], [1208, 'contradiction'], [1209, 'contradiction'], [1210, 'contradiction'], [1211, 'contradiction'], [1212, 'contradiction'], [1213, 'contradiction'], [1214, 'contradiction'], [1215, 'contradiction'], [1216, 'contradiction'], [1217, 'contradiction'], [1218, 'contradiction'], [1219, 'contradiction'], [1220, 'contradiction'], [1221, 'contradiction'], [1222, 'contradiction'], [1223, 'contradiction'], [1224, 'contradiction'], [1225, 'contradiction'], [1226, 'contradiction'], [1227, 'contradiction'], [1228, 'contradiction'], [1229, 'contradiction'], [1230, 'contradiction'], [1231, 'contradiction'], [1232, 'contradiction'], [1233, 'contradiction'], [1234, 'contradiction'], [1235, 'contradiction'], [1236, 'contradiction'], [1237, 'contradiction'], [1238, 'contradiction'], [1239, 'contradiction'], [1240, 'contradiction'], [1241, 'contradiction'], [1242, 'contradiction'], [1243, 'contradiction'], [1244, 'contradiction'], [1245, 'contradiction'], [1246, 'contradiction'], [1247, 'contradiction'], [1248, 'contradiction'], [1249, 'contradiction'], [1250, 'contradiction'], [1251, 'contradiction'], [1252, 'contradiction'], [1253, 'contradiction'], [1254, 'contradiction'], [1255, 'contradiction'], [1256, 'contradiction'], [1257, 'contradiction'], [1258, 'contradiction'], [1259, 'contradiction'], [1260, 'contradiction'], [1261, 'contradiction'], [1262, 'contradiction'], [1263, 'contradiction'], [1264, 'contradiction'], [1265, 'contradiction'], [1266, 'contradiction'], [1267, 'contradiction'], [1268, 'contradiction'], [1269, 'contradiction'], [1270, 'contradiction'], [1271, 'contradiction'], [1272, 'contradiction'], [1273, 'contradiction'], [1274, 'contradiction'], [1275, 'contradiction'], [1276, 'contradiction'], [1277, 'contradiction'], [1278, 'contradiction'], [1279, 'contradiction'], [1280, 'contradiction'], [1281, 'contradiction'], [1282, 'contradiction'], [1283, 'contradiction'], [1284, 'contradiction'], [1285, 'contradiction'], [1286, 'contradiction'], [1287, 'contradiction'], [1288, 'contradiction'], [1289, 'contradiction'], [1290, 'contradiction'], [1291, 'contradiction'], [1292, 'contradiction'], [1293, 'contradiction'], [1294, 'contradiction'], [1295, 'contradiction'], [1296, 'contradiction'], [1297, 'contradiction'], [1298, 'contradiction'], [1299, 'contradiction'], [1300, 'contradiction'], [1301, 'contradiction'], [1302, 'contradiction'], [1303, 'contradiction'], [1304, 'contradiction'], [1305, 'contradiction'], [1306, 'contradiction'], [1307, 'contradiction'], [1308, 'contradiction'], [1309, 'contradiction'], [1310, 'contradiction'], [1311, 'contradiction'], [1312, 'contradiction'], [1313, 'contradiction'], [1314, 'contradiction'], [1315, 'contradiction'], [1316, 'contradiction'], [1317, 'contradiction'], [1318, 'contradiction'], [1319, 'contradiction'], [1320, 'contradiction'], [1321, 'contradiction'], [1322, 'contradiction'], [1323, 'contradiction'], [1324, 'contradiction'], [1325, 'contradiction'], [1326, 'contradiction'], [1327, 'contradiction'], [1328, 'contradiction'], [1329, 'contradiction'], [1330, 'contradiction'], [1331, 'contradiction'], [1332, 'contradiction'], [1333, 'contradiction'], [1334, 'contradiction'], [1335, 'contradiction'], [1336, 'contradiction'], [1337, 'contradiction'], [1338, 'contradiction'], [1339, 'contradiction'], [1340, 'contradiction'], [1341, 'contradiction'], [1342, 'contradiction'], [1343, 'contradiction'], [1344, 'contradiction'], [1345, 'contradiction'], [1346, 'contradiction'], [1347, 'contradiction'], [1348, 'contradiction'], [1349, 'contradiction'], [1350, 'contradiction'], [1351, 'contradiction'], [1352, 'contradiction'], [1353, 'contradiction'], [1354, 'contradiction'], [1355, 'contradiction'], [1356, 'contradiction'], [1357, 'contradiction'], [1358, 'contradiction'], [1359, 'contradiction'], [1360, 'contradiction'], [1361, 'contradiction'], [1362, 'contradiction'], [1363, 'contradiction'], [1364, 'contradiction'], [1365, 'contradiction'], [1366, 'contradiction'], [1367, 'contradiction'], [1368, 'contradiction'], [1369, 'contradiction'], [1370, 'contradiction'], [1371, 'contradiction'], [1372, 'contradiction'], [1373, 'contradiction'], [1374, 'contradiction'], [1375, 'contradiction'], [1376, 'contradiction'], [1377, 'contradiction'], [1378, 'contradiction'], [1379, 'contradiction'], [1380, 'contradiction'], [1381, 'contradiction'], [1382, 'contradiction'], [1383, 'contradiction'], [1384, 'contradiction'], [1385, 'contradiction'], [1386, 'contradiction'], [1387, 'contradiction'], [1388, 'contradiction'], [1389, 'contradiction'], [1390, 'contradiction'], [1391, 'contradiction'], [1392, 'contradiction'], [1393, 'contradiction'], [1394, 'contradiction'], [1395, 'contradiction'], [1396, 'contradiction'], [1397, 'contradiction'], [1398, 'contradiction'], [1399, 'contradiction'], [1400, 'contradiction'], [1401, 'contradiction'], [1402, 'contradiction'], [1403, 'contradiction'], [1404, 'contradiction'], [1405, 'contradiction'], [1406, 'contradiction'], [1407, 'contradiction'], [1408, 'contradiction'], [1409, 'contradiction'], [1410, 'contradiction'], [1411, 'contradiction'], [1412, 'contradiction'], [1413, 'contradiction'], [1414, 'contradiction'], [1415, 'contradiction'], [1416, 'contradiction'], [1417, 'contradiction'], [1418, 'contradiction'], [1419, 'contradiction'], [1420, 'contradiction'], [1421, 'contradiction'], [1422, 'contradiction'], [1423, 'contradiction'], [1424, 'contradiction'], [1425, 'contradiction'], [1426, 'contradiction'], [1427, 'contradiction'], [1428, 'contradiction'], [1429, 'contradiction'], [1430, 'contradiction'], [1431, 'contradiction'], [1432, 'contradiction'], [1433, 'contradiction'], [1434, 'contradiction'], [1435, 'contradiction'], [1436, 'contradiction'], [1437, 'contradiction'], [1438, 'contradiction'], [1439, 'contradiction'], [1440, 'contradiction'], [1441, 'contradiction'], [1442, 'contradiction'], [1443, 'contradiction'], [1444, 'contradiction'], [1445, 'contradiction'], [1446, 'contradiction'], [1447, 'contradiction'], [1448, 'contradiction'], [1449, 'contradiction'], [1450, 'contradiction'], [1451, 'contradiction'], [1452, 'contradiction'], [1453, 'contradiction'], [1454, 'contradiction'], [1455, 'contradiction'], [1456, 'contradiction'], [1457, 'contradiction'], [1458, 'contradiction'], [1459, 'contradiction'], [1460, 'contradiction'], [1461, 'contradiction'], [1462, 'contradiction'], [1463, 'contradiction'], [1464, 'contradiction'], [1465, 'contradiction'], [1466, 'contradiction'], [1467, 'contradiction'], [1468, 'contradiction'], [1469, 'contradiction'], [1470, 'contradiction'], [1471, 'contradiction'], [1472, 'contradiction'], [1473, 'contradiction'], [1474, 'contradiction'], [1475, 'contradiction'], [1476, 'contradiction'], [1477, 'contradiction'], [1478, 'contradiction'], [1479, 'contradiction'], [1480, 'contradiction'], [1481, 'contradiction'], [1482, 'contradiction'], [1483, 'contradiction'], [1484, 'contradiction'], [1485, 'contradiction'], [1486, 'contradiction'], [1487, 'contradiction'], [1488, 'contradiction'], [1489, 'contradiction'], [1490, 'contradiction'], [1491, 'contradiction'], [1492, 'contradiction'], [1493, 'contradiction'], [1494, 'contradiction'], [1495, 'contradiction'], [1496, 'contradiction'], [1497, 'contradiction'], [1498, 'contradiction'], [1499, 'contradiction'], [1500, 'contradiction'], [1501, 'contradiction'], [1502, 'contradiction'], [1503, 'contradiction'], [1504, 'contradiction'], [1505, 'contradiction'], [1506, 'contradiction'], [1507, 'contradiction'], [1508, 'contradiction'], [1509, 'contradiction'], [1510, 'contradiction'], [1511, 'contradiction'], [1512, 'contradiction'], [1513, 'contradiction'], [1514, 'contradiction'], [1515, 'contradiction'], [1516, 'contradiction'], [1517, 'contradiction'], [1518, 'contradiction'], [1519, 'contradiction'], [1520, 'contradiction'], [1521, 'contradiction'], [1522, 'contradiction'], [1523, 'contradiction'], [1524, 'contradiction'], [1525, 'contradiction'], [1526, 'contradiction'], [1527, 'contradiction'], [1528, 'contradiction'], [1529, 'contradiction'], [1530, 'contradiction'], [1531, 'contradiction'], [1532, 'contradiction'], [1533, 'contradiction'], [1534, 'contradiction'], [1535, 'contradiction'], [1536, 'contradiction'], [1537, 'contradiction'], [1538, 'contradiction'], [1539, 'contradiction'], [1540, 'contradiction'], [1541, 'contradiction'], [1542, 'contradiction'], [1543, 'contradiction'], [1544, 'contradiction'], [1545, 'contradiction'], [1546, 'contradiction'], [1547, 'contradiction'], [1548, 'contradiction'], [1549, 'contradiction'], [1550, 'contradiction'], [1551, 'contradiction'], [1552, 'contradiction'], [1553, 'contradiction'], [1554, 'contradiction'], [1555, 'contradiction'], [1556, 'contradiction'], [1557, 'contradiction'], [1558, 'contradiction'], [1559, 'contradiction'], [1560, 'contradiction'], [1561, 'contradiction'], [1562, 'contradiction'], [1563, 'contradiction'], [1564, 'contradiction'], [1565, 'contradiction'], [1566, 'contradiction'], [1567, 'contradiction'], [1568, 'contradiction'], [1569, 'contradiction'], [1570, 'contradiction'], [1571, 'contradiction'], [1572, 'contradiction'], [1573, 'contradiction'], [1574, 'contradiction'], [1575, 'contradiction'], [1576, 'contradiction'], [1577, 'contradiction'], [1578, 'contradiction'], [1579, 'contradiction'], [1580, 'contradiction'], [1581, 'contradiction'], [1582, 'contradiction'], [1583, 'contradiction'], [1584, 'contradiction'], [1585, 'contradiction'], [1586, 'contradiction'], [1587, 'contradiction'], [1588, 'contradiction'], [1589, 'contradiction'], [1590, 'contradiction'], [1591, 'contradiction'], [1592, 'contradiction'], [1593, 'contradiction'], [1594, 'contradiction'], [1595, 'contradiction'], [1596, 'contradiction'], [1597, 'contradiction'], [1598, 'contradiction'], [1599, 'contradiction'], [1600, 'contradiction'], [1601, 'contradiction'], [1602, 'contradiction'], [1603, 'contradiction'], [1604, 'contradiction'], [1605, 'contradiction'], [1606, 'contradiction'], [1607, 'contradiction'], [1608, 'contradiction'], [1609, 'contradiction'], [1610, 'contradiction'], [1611, 'contradiction'], [1612, 'contradiction'], [1613, 'contradiction'], [1614, 'contradiction'], [1615, 'contradiction'], [1616, 'contradiction'], [1617, 'contradiction'], [1618, 'contradiction'], [1619, 'contradiction'], [1620, 'contradiction'], [1621, 'contradiction'], [1622, 'contradiction'], [1623, 'contradiction'], [1624, 'contradiction'], [1625, 'contradiction'], [1626, 'contradiction'], [1627, 'contradiction'], [1628, 'contradiction'], [1629, 'contradiction'], [1630, 'contradiction'], [1631, 'contradiction'], [1632, 'contradiction'], [1633, 'contradiction'], [1634, 'contradiction'], [1635, 'contradiction'], [1636, 'contradiction'], [1637, 'contradiction'], [1638, 'contradiction'], [1639, 'contradiction'], [1640, 'contradiction'], [1641, 'contradiction'], [1642, 'contradiction'], [1643, 'contradiction'], [1644, 'contradiction'], [1645, 'contradiction'], [1646, 'contradiction'], [1647, 'contradiction'], [1648, 'contradiction'], [1649, 'contradiction'], [1650, 'contradiction'], [1651, 'contradiction'], [1652, 'contradiction'], [1653, 'contradiction'], [1654, 'contradiction'], [1655, 'contradiction'], [1656, 'contradiction'], [1657, 'contradiction'], [1658, 'contradiction'], [1659, 'contradiction'], [1660, 'contradiction'], [1661, 'contradiction'], [1662, 'contradiction'], [1663, 'contradiction'], [1664, 'contradiction'], [1665, 'contradiction']]\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: './result/submission08.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25952/1381421126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./result/submission08.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: './result/submission08.csv'"
     ]
    }
   ],
   "source": [
    "test_label = label_to_num(test['label'].values)\n",
    "\n",
    "tokenized_test = tokenizer(\n",
    "    list(test['premise']),\n",
    "    list(test['hypothesis']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "test_dataset = BERTDataset(tokenized_test, test_label)\n",
    "\n",
    "print(test_dataset.__len__())\n",
    "print(test_dataset.__getitem__(1665))\n",
    "print(tokenizer.decode(test_dataset.__getitem__(6)['input_ids']))\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "output_pred = []\n",
    "output_prob = []\n",
    "\n",
    "for i, data in enumerate(tqdm(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=data['input_ids'].to(device),\n",
    "            attention_mask=data['attention_mask'].to(device),\n",
    "            token_type_ids=data['token_type_ids'].to(device)\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)\n",
    "\n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
    "print(pred_answer)\n",
    "\n",
    "def num_to_label(label):\n",
    "    label_dict = {0: \"entailment\", 1: \"contradiction\", 2: \"neutral\"}\n",
    "    str_label = []\n",
    "\n",
    "    for i, v in enumerate(label):\n",
    "        str_label.append([i,label_dict[v]])\n",
    "    \n",
    "    return str_label\n",
    "\n",
    "answer = num_to_label(pred_answer)\n",
    "print(answer)\n",
    "\n",
    "df = pd.DataFrame(answer, columns=['index', 'label'])\n",
    "\n",
    "df.to_csv('./result/submission08.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3649fa4b9a6d1d3559da311d910871468d04a06554bfd4e3300b254822ca1a7a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
