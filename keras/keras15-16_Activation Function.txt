## [Activation Function] : linear / sigmoid / softmax  

1. Sigmoid function   - loss : 'binary_crossentropy'
Sigmoid의 출력 범위는 [0, 1] 사이
s자와 유사한 완만한 시그모이드 커브형태를 보이는 함수
"S" 형태의 포함될 커브를 생성. 시그모이드 함수는 Logistic 함수라고 불리기도 하며, x의 값에 따라 0~1의 값을 출력하는 S자형 함수

대표적인 Logistic 함수
function을 사용할 때보다 훨씬 많은 일을 할 수 있기 때문에 sigmoid함수는 입력단어나 hidden layer에 주로 사용되며, 출력단에서도 사용됨

linear에서 함께 사용할 수 있음 -> hidden layer부분에서 사용하여 효율 증대
주로 마지막 단계에서 사용하는 것이 가장 효율이 좋음(이진분류로 나누는 것이기 때문에)

과적합되지 않도록 유의할 것!

[참고]
https://codedragon.tistory.com/9428
https://han-py.tistory.com/266

2. Softmax function   - loss : 'categorical_crossentropy'

'다중 분류' 로지스틱 회귀 모델에 사용
Softmax 출력의 각 원소는 0.0 이상 1.0 이하의 실수. 그리고 노드의 출력을 모두 합한 값이 항상 1
Softmax 함수는 모든 출력 값의 상대적인 크기를 고려한 값을 출력하기 때문에, 다 범주 분류 신경망에 적합함

- 다중분류모델 최종 node의 갯수는 label의 갯수와 동일해야함

<ONE-HOT-ENCODING>

ONE-HOT-ENCODING은 softmax사용시 가장 높은 확률의 값을 1로 설정하고 나머지는 다 0으로 만드는 기법
다중분류에서는 input시 y값의 ONE-HOT-ENCODING으로 결과값의 컬럼갯수를 맞춰줌. 쏠림현상 방지
다중분류라고 판단이 되면 x값을 인풋하기전에 y값의 컬럼갯수를 맞춰주어야함.

[참고]
https://wikidocs.net/35476
https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=rian4u&logNo=221398406858
https://devuna.tistory.com/67  -> [pandas] pd.get_dummies() : 데이터전처리/가변수 만들기

## [metrics] - 평가지표

metrics는 어떤 모델을 평가(Evaluate)하기 위해서 사용하는 값

evaluate 했을때 출력되는 list의 첫번째 값은 'loss'값, 두번째 값은 'metrics'의 평가값
모델 가중치의 업데이트에는 영향을 미치지 않음

loss: 손실함수. 훈련셋과 연관. 훈련에 사용 -> 중요한 값 
metric: 평가지표. 검증셋과 연관. 훈련 과정을 모니터링하는데 사용

[참고]
=======
## [Activation Function] : linear / sigmoid / softmax  

1. Sigmoid function   - loss : 'binary_crossentropy'
Sigmoid의 출력 범위는 [0, 1] 사이
s자와 유사한 완만한 시그모이드 커브형태를 보이는 함수
"S" 형태의 포함될 커브를 생성. 시그모이드 함수는 Logistic 함수라고 불리기도 하며, x의 값에 따라 0~1의 값을 출력하는 S자형 함수

대표적인 Logistic 함수
function을 사용할 때보다 훨씬 많은 일을 할 수 있기 때문에 sigmoid함수는 입력단어나 hidden layer에 주로 사용되며, 출력단에서도 사용됨

linear에서 함께 사용할 수 있음 -> hidden layer부분에서 사용하여 효율 증대
주로 마지막 단계에서 사용하는 것이 가장 효율이 좋음(이진분류로 나누는 것이기 때문에)

과적합되지 않도록 유의할 것!

[참고]
https://codedragon.tistory.com/9428
https://han-py.tistory.com/266

2. Softmax function   - loss : 'categorical_crossentropy'

'다중 분류' 로지스틱 회귀 모델에 사용
Softmax 출력의 각 원소는 0.0 이상 1.0 이하의 실수. 그리고 노드의 출력을 모두 합한 값이 항상 1
Softmax 함수는 모든 출력 값의 상대적인 크기를 고려한 값을 출력하기 때문에, 다 범주 분류 신경망에 적합함

- 다중분류모델 최종 node의 갯수는 label의 갯수와 동일해야함

<ONE-HOT-ENCODING>

ONE-HOT-ENCODING은 softmax사용시 가장 높은 확률의 값을 1로 설정하고 나머지는 다 0으로 만드는 기법
다중분류에서는 input시 y값의 ONE-HOT-ENCODING으로 결과값의 컬럼갯수를 맞춰줌. 쏠림현상 방지
다중분류라고 판단이 되면 x값을 인풋하기전에 y값의 컬럼갯수를 맞춰주어야함.

[참고]
https://wikidocs.net/35476
https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=rian4u&logNo=221398406858
https://devuna.tistory.com/67  -> [pandas] pd.get_dummies() : 데이터전처리/가변수 만들기

## [metrics] - 평가지표

metrics는 어떤 모델을 평가(Evaluate)하기 위해서 사용하는 값

evaluate 했을때 출력되는 list의 첫번째 값은 'loss'값, 두번째 값은 'metrics'의 평가값
모델 가중치의 업데이트에는 영향을 미치지 않음

loss: 손실함수. 훈련셋과 연관. 훈련에 사용 -> 중요한 값 
metric: 평가지표. 검증셋과 연관. 훈련 과정을 모니터링하는데 사용

[참고]
https://needjarvis.tistory.com/568